// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Implements a Wasm interpreter by running handwritten x86-64 interpreter loop.
def GLOBAL_BUFFER_MARKER = 0xFACEBEEF778899AAuL;
def INTERPRETER_CODE_MARKER = 0xFFAACCEEuL;
def GLOBAL_BUFFER_HEADER_SIZE = 64;
component X86_64Interpreter {
	var interpreterCode: X86_64InterpreterCode; // Dynamically-generated interpreter code.
	var asmEntry: (/*wf: */ WasmFunction, /*sp: */ Pointer) -> (ExecState, TrapReason);
	var valueStack: ValueStack;
	var dispatchTable: Pointer;
	var frame: Frame;

	def run(f: Function, args: Array<Value>) -> Result {
		if (interpreterCode == null) genInterpreterCode();
		// Unpack arguments into value stack format.
		var fp = valueStack.sp;
		if (args != null) for (v in args) valueStack.push(v);

		// Call the main loop which handles tail calls.
		var state = runWithTailCalls(f);

		// Unpack state into interpreter result.
		match (state.0) {
			STOPPED,
			RUN_FAST, RUN_SLOW, PROBING => return Result.Trap(TrapReason.UNIMPLEMENTED);
			TRAPPED => return Result.Trap(state.1);
			TRAPPING => return Result.Trap(state.1);
			FINISHED => return popResult(f.sig);
		}
	}
	def serializeInterpreterCodeIntoExecutable(executable: Array<byte>) -> bool {
		if (interpreterCode == null) genInterpreterCode();
		// try to find {global_buffer} in {data}
		var d = DataReader.new(executable);
		// the global buffer contents will be at the same page alignment in the executable
		var page_offset = int.!((Pointer.atContents(global_buffer) - Pointer.NULL) & (PAGE_SIZE - 1));
		var found = -1;
		for (pos = page_offset; pos < d.limit; pos += PAGE_SIZE) {
			var val = d.at(pos).read_u64();
			if (val == GLOBAL_BUFFER_MARKER) { found = pos; break; }
		}
		if (found < 0) return false;
		// Write the executable code and the offsets of {InterpreterCode} into the executable.
		var w = DataWriter.new().reset(executable, found, found);
		w.puta(global_buffer); // write machine code
		interpreterCode.serialize(w.at(found + 8)); // write interpreter offsets
		return true;
	}
	private def genInterpreterCode() {
		valueStack = ValueStack.new(EngineOptions.stackSize);
		var start = System.ticksUs();
		interpreterCode = deserializeOrGenerateInterpreterCode();
		RiRuntime.registerUserCode(interpreterCode);
		var diff = System.ticksUs() - start;
		if (Trace.interpreter) Trace.OUT.put1("Created interpreter in %d \xCE\xBCs.\n", diff).outln();
		asmEntry = CiRuntime.forgeClosure<
			X86_64Interpreter,				// closure type
			(/*wf: */ WasmFunction, /*sp: */ Pointer),	// parameter types
			(ExecState, TrapReason)>(			// return types
				interpreterCode.start + interpreterCode.v3EntryOffset, this);
		dispatchTable = interpreterCode.start +
			if(Execute.probes.elem != null,
				interpreterCode.probedDispatchTableOffset,
				interpreterCode.fastDispatchTableOffset);
	}
	def onProbeEnable() {
		if (interpreterCode != null) dispatchTable = interpreterCode.start + interpreterCode.probedDispatchTableOffset;
	}
	def onProbeDisable() {
		if (interpreterCode != null) dispatchTable = interpreterCode.start + interpreterCode.fastDispatchTableOffset;
	}
	def getTopFrame() -> Frame {
		return if(frame == null, Frame.new(), frame);
	}
	def getCallStack() -> ArrayStack<Frame> {
		return ArrayStack.new();
	}
	def getCallDepth() -> int {
		return 0; // TODO: get wasm call stack depth
	}
	def reset() {
		if (valueStack != null) valueStack.sp = valueStack.mapping.range.start;
		frame = null;
	}
	private def runWithTailCalls(f: Function) -> (ExecState, TrapReason) {
		var state = (ExecState.FINISHED, TrapReason.NONE);
		while (true) { // handle repeated tail calls
			var result: HostResult;
			match (f) {
				wf: WasmFunction => {
					var sp = valueStack.sp;
					state = invoke(wf, sp);
					if (state.0 == ExecState.FINISHED) {
						valueStack.sp = sp + ((wf.sig.results.length - wf.sig.params.length) * valueStack.valuerep.slot_size);
					}
					break;
				}
				hf: HostFunction0 => {
					if (Trace.interpreter) Execute.traceCallHostFunction(hf);
					result = hf.invoke0();
				}
				hf: HostFunction1 => {
					if (Trace.interpreter) Execute.traceCallHostFunction(hf);
					var a0 = valueStack.pop(hf.sig.params[0]);
					result = hf.invoke1(a0);
				}
				hf: HostFunction2 => {
					if (Trace.interpreter) Execute.traceCallHostFunction(hf);
					var a1 = valueStack.pop(hf.sig.params[1]);
					var a0 = valueStack.pop(hf.sig.params[0]);
					result = hf.invoke2(a0, a1);
				}
				hf: HostFunction3 => {
					if (Trace.interpreter) Execute.traceCallHostFunction(hf);
					var a2 = valueStack.pop(hf.sig.params[2]);
					var a1 = valueStack.pop(hf.sig.params[1]);
					var a0 = valueStack.pop(hf.sig.params[0]);
					result = hf.invoke3(a0, a1, a2);
				}
				hf: HostFunctionN => {
					if (Trace.interpreter) Execute.traceCallHostFunction(hf);
					var aN = valueStack.popN(hf.sig.params);
					result = hf.invokeN(aN);
				}
			}
			match (result) {
				Trap(reason) => {
					state = (ExecState.TRAPPED, reason);
					break;
				}
				Error(msg) => {
					state = (ExecState.TRAPPED, TrapReason.ERROR);
					Execute.error_msg = msg;
					break;
				}
				Value0 => {
					break;
				}
				Value1(val) => {
					valueStack.push(val);
					break;
				}
				ValueN(vals) => {
					for (a in vals) valueStack.push(a);
					break;
				}
				TailCall(target, args) => {
					for (a in args) valueStack.push(a);
					f = target;
					continue; // continue with next tail call
				}
			}
		}
		return state;
	}
	private def invoke(wf: WasmFunction, sp: Pointer) -> (ExecState, TrapReason) {
		if (wf.decl.code.target_code.spc_entry != Pointer.NULL) {
			return Target.callSpcEntry(wf);
		}
		return asmEntry(wf, sp);
	}
	private def popResult(sig: SigDecl) -> Result {
		var rt = sig.results;
		var r = Array<Value>.new(rt.length);
		for (i = r.length - 1; i >= 0; i--) r[i] = valueStack.pop(rt[i]);
		return Result.Value(r);
	}
	// =======================================================================================
	// INTERPRETER RUNTIME CALLBACKS
	// =======================================================================================
	def runtime_callHost(f: Function) -> (ExecState, TrapReason) {
		var state = runWithTailCalls(f);
		return state;
	}
	def runtime_MEMORY_GROW(instance: Instance, index: u32) {
		var memory = instance.memories[index];
		var pages = Values.v_u(valueStack.pop(ValueType.I32));
		var result = memory.grow(pages);
		valueStack.push(Values.i_v(result));
	}
	def runtime_MEMORY_INIT(instance: Instance, dindex: u32, mindex: u32) -> (ExecState, TrapReason) {
		var memory = instance.memories[mindex];
		var ddecl = if(!instance.dropped_data[dindex], instance.module.data[int.!(dindex)]);
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		var t = memory.copyIn(dst_offset, if(ddecl != null, ddecl.data), src_offset, size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def runtime_MEMORY_COPY(instance: Instance, mindex1: u32, mindex2: u32) -> (ExecState, TrapReason) { // XXX: inline
		var dst = instance.memories[mindex1];
		var src = instance.memories[mindex2];
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		var t = dst.copyM(dst_offset, src, src_offset, size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def runtime_MEMORY_FILL(instance: Instance, mindex: u32) -> (ExecState, TrapReason) {
		var memory = instance.memories[mindex];
		var size = valueStack.popu();
		var val = valueStack.popu();
		var dest = valueStack.popu();
		var t = memory.fill(dest, u8.view(val), size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def runtime_GLOBAL_GET(instance: Instance, index: u32) { // XXX: inline when Value rep known
		var val = instance.globals[index].value;
		valueStack.push(val);
	}
	def runtime_GLOBAL_SET(instance: Instance, index: u32) { // XXX: inline when Value rep known
		var g = instance.globals[index];
		var val = valueStack.pop(g.valtype);
		g.value = val;
	}
	def runtime_TABLE_GET(instance: Instance, index: u32) -> (ExecState, TrapReason) { // XXX: inline when Value rep known
		var table = instance.tables[index];
		var elem = Values.v_u(valueStack.pop(ValueType.I32));
		if (elem >= table.elems.length) return (ExecState.TRAPPED, TrapReason.TABLE_OUT_OF_BOUNDS);
		var val = table.elems[elem];
		valueStack.push(val);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def runtime_TABLE_SET(instance: Instance, index: u32) -> (ExecState, TrapReason) {
		var table = instance.tables[index];
		var val = valueStack.pop(table.elemtype);
		var elem = Values.v_u(valueStack.pop(ValueType.I32));
		if (elem >= table.elems.length) return (ExecState.TRAPPED, TrapReason.TABLE_OUT_OF_BOUNDS);
		table[int.view(elem)] = val;
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def runtime_TABLE_INIT(instance: Instance, eindex: u32, tindex: u32) -> (ExecState, TrapReason) {
		var elem = if (!instance.dropped_elems[eindex], instance.module.elems[int.!(eindex)]);
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var src_offset = valueStack.popu();
		var dst_offset = valueStack.popu();
		var t = table.copyE(instance, dst_offset, elem, src_offset, size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def runtime_TABLE_COPY(instance: Instance, t1: u32, t2: u32) -> (ExecState, TrapReason) {
		var dst = instance.tables[t1];
		var src = instance.tables[t2];
		var size = valueStack.popu(), src_offset = valueStack.popu(), dst_offset = valueStack.popu();
		var t = dst.copyT(dst_offset, src, src_offset, size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def runtime_TABLE_GROW(instance: Instance, tindex: u32) {
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var val = valueStack.pop(table.elemtype);
		var r = table.grow(size, val);
		valueStack.push(Values.i_v(r));
	}
	def runtime_TABLE_FILL(instance: Instance, tindex: u32) -> (ExecState, TrapReason) { // XXX: inline when Value rep known
		var table = instance.tables[tindex];
		var size = valueStack.popu();
		var val = valueStack.pop(table.elemtype);
		var dest = valueStack.popu();
		var t = table.fill(dest, val, size);
		if (t != TrapReason.NONE) return (ExecState.TRAPPED, t);
		return (ExecState.FINISHED, TrapReason.NONE);
	}
	def runtime_PROBE_loop(instance: Instance) -> (ExecState, TrapReason) {
		var sp = CiRuntime.callerSp();
		frame = materializeFrame(sp);

		if (Execute.fireProbes(frame.func, frame.pc)) return (ExecState.FINISHED, TrapReason.NONE);
		return (Execute.state, Execute.trap_reason);
	}
	def runtime_PROBE_instr(instance: Instance) -> (ExecState, TrapReason) {
		var sp = CiRuntime.callerSp();
		frame = materializeFrame(sp);

		if (Execute.fireProbesAt(frame.func, frame.pc)) return (ExecState.FINISHED, TrapReason.NONE);
		return (Execute.state, Execute.trap_reason);
	}
	private def materializeFrame(sp: Pointer) -> Frame {
		var func = (sp + IVar.WASM_FUNC.frameOffset).load<WasmFunction>();
		var ip   = (sp + IVar.IP.frameOffset).load<Pointer>();
		var code = Pointer.atContents((sp + IVar.CODE.frameOffset).load<Array<byte>>());
		var xip  = (sp + IVar.STP.frameOffset).load<Pointer>();

		frame = Frame.new();
		frame.func = func;
		frame.pc = int.!(ip - code - 1);
		frame.xip = int.!(xip - Pointer.atContents(func.decl.code.sidetable));
		return frame;
	}
}

// Signal-handling for traps
def ucontext_rip_offset = 168;
def ucontext_rsp_offset = 160;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;

// Implements the RiUserCode interface in order to add generated machine code to the V3 runtime.
// Also stores several important offsets needed in handling signals.
class X86_64InterpreterCode extends RiUserCode {
	def frameSize = IVar.INSTANCE.frameOffset + Pointer.SIZE;
	var fastDispatchTableOffset: int;	// dispatch table when probes disabled
	var probedDispatchTableOffset: int;	// dispatch table when probes enabled
	var v3EntryOffset: int;			// entry from V3 calling code
	var oobMemoryHandlerOffset: int;	// handler for signals caused by OOB memory access
	var divZeroHandlerOffset: int;		// handler for signals caused by divide by zero
	var stackOverflowHandlerOffset: int;	// handler for signals caused by (value- or call-) stack overflow
	var codeEnd: int;			// end of all code
	var buf = StringBuilder.new().grow(128);  // avoid allocations when describing frames

	new(start: Pointer, end: Pointer) super(start, end) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg = "\tin [fast-int] ";
		out(msg, 0, msg.length);
		var instance = (sp + IVar.INSTANCE.frameOffset).load<Instance>();
		var func = (sp + IVar.FUNC_DECL.frameOffset).load<FuncDecl>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		func.render(instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}

	// Called from V3 runtime for a frame where {ip} is in interpreter code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}

	// Called from V3 runtime when the garbage collector needs to scan an interpreter stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle code and interior pointers
		var code_loc = (sp + IVar.CODE.frameOffset);
		var code = code_loc.load<Pointer>();
		var ip_delta = (sp + IVar.IP.frameOffset).load<Pointer>() - code;
		var eip_delta = (sp + IVar.EIP.frameOffset).load<Pointer>() - code;
		RiGc.scanRoot(code_loc);
		var new_code = code_loc.load<Pointer>();
		if (new_code != code) {
			(sp + IVar.IP.frameOffset).store<Pointer>(new_code + ip_delta);
			(sp + IVar.EIP.frameOffset).store<Pointer>(new_code + eip_delta);
		}

		// Handle sidetable and interior pointer
		var sidetable_loc = (sp + IVar.SIDETABLE.frameOffset);
		var sidetable = sidetable_loc.load<Pointer>();
		var xip_delta = (sp + IVar.STP.frameOffset).load<Pointer>() - sidetable;
		RiGc.scanRoot(sidetable_loc);
		var new_sidetable = sidetable_loc.load<Pointer>();
		if (new_sidetable != sidetable) {
			(sp + IVar.STP.frameOffset).store<Pointer>(new_sidetable + xip_delta);
		}

		// Handle other roots in the frame
		RiGc.scanRoot(sp + IVar.FUNC_DECL.frameOffset);
		RiGc.scanRoot(sp + IVar.INSTANCE.frameOffset);
	}

	// Called from V3 runtime to handle an OS-level signal that occurred while {ip} was in interpreter code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		var pip = ucontext + ucontext_rip_offset;
		var ip = pip.load<Pointer>();
		if (Trace.interpreter) {
			Trace.OUT.put2("  !signal %d in interpreter @ 0x%x", signum, ip - Pointer.NULL).outln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				pip.store<Pointer>(start + divZeroHandlerOffset);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				if (RedZones.isInRedZone(addr)) {
					pip.store<Pointer>(start + stackOverflowHandlerOffset);
					return true;
				}
				pip.store<Pointer>(start + oobMemoryHandlerOffset);
				return true;
			}
		}
		return true;
	}
	// Initializes the interpreter code object from serialized memory (e.g. global buffer).
	def deserialize(d: DataReader) -> bool {
		if (d.read_u32() != INTERPRETER_CODE_MARKER) return false;
		fastDispatchTableOffset = int.view(d.read_u32());
		probedDispatchTableOffset = int.view(d.read_u32());
		v3EntryOffset = int.view(d.read_u32());
		oobMemoryHandlerOffset = int.view(d.read_u32());
		divZeroHandlerOffset = int.view(d.read_u32());
		stackOverflowHandlerOffset = int.view(d.read_u32());
		codeEnd = int.view(d.read_u32());
		return true;
	}
	// Writes the interpreter code object fields to serialized memory (e.g. a file with global buffer).
	def serialize(w: DataWriter) {
		w.put_b32(int.!(INTERPRETER_CODE_MARKER));
		w.put_b32(fastDispatchTableOffset);
		w.put_b32(probedDispatchTableOffset);
		w.put_b32(v3EntryOffset);
		w.put_b32(oobMemoryHandlerOffset);
		w.put_b32(divZeroHandlerOffset);
		w.put_b32(stackOverflowHandlerOffset);
		w.put_b32(codeEnd);
	}
}

// Implements a value stack using raw (Pointer) memory, with explicitly tagged values.
// Maps a value stack red zone at the end to catch and report stack overflow.
class ValueStack {
	def valuerep = Target.tagging;
	def size: u32;
	def mapping = Mmap.reserve(size, Mmap.PROT_READ | Mmap.PROT_WRITE);
	var sp: Pointer;

	new(size) {
		if (mapping == null) fatal("out of memory allocating value stack");
		sp = mapping.range.start;
		def PAGE_SIZE = 4096u;
		var ok = RedZones.addRedZone(mapping, size - PAGE_SIZE, PAGE_SIZE);
		if (!ok) fatal("could not protect value stack red zone");
		if (valuerep.tagged) RiGc.registerScanner(this, ValueStack.scan);
	}
	def push(v: Value) {
		match (v) {
			Ref(obj) => pushPair(BpTypeCode.REF_NULL.code, obj);
			I31(val) => pushPair(BpTypeCode.I31REF.code, u64.view(val));
			I32(val) => pushPair(BpTypeCode.I32.code, u64.view(val));
			I64(val) => pushPair(BpTypeCode.I64.code, u64.view(val));
			F32(bits) => pushPair(BpTypeCode.F32.code, u64.view(bits));
			F64(bits) => pushPair(BpTypeCode.F64.code, u64.view(bits));
			V128(low, high) => pushPair(BpTypeCode.V128.code, u64.view(low)); // TODO: full simd value
		}
	}
	def popN(t: Array<ValueType>) -> Array<Value> {
		var r = Array<Value>.new(t.length);
		for (j = t.length - 1; j >= 0; j--) r[j] = pop(t[j]);
		return r;
	}
	def pop(t: ValueType) -> Value {
		match (t) {
			I32 => return Value.I32(popb32(BpTypeCode.I32.code));
			I64 => return Value.I64(popb64(BpTypeCode.I64.code));
			F32 => return Value.F32(popb32(BpTypeCode.F32.code));
			F64 => return Value.F64(popb64(BpTypeCode.F64.code));
			Host => return Value.Ref(popObject());
			Ref(nullable, heap) => match(heap) {
				ANY, EXTERN, EQ, I31 => {
					if (peekTag() == BpTypeCode.I31REF.code) return Value.I31(u31.view(popb32(BpTypeCode.I31REF.code)));
					else return Value.Ref(popObject());
				}
				Func => return Value.Ref(popFunction());
				Rtt => return Value.Ref(popRtt());
				_ => return Value.Ref(popObject());
			}
			_ => fatal(Strings.format1("unexpected type: %s", t.name));
		}
		return Value.Ref(null);
	}
	def popu() -> u32 {
		return popb32(BpTypeCode.I32.code);
	}
	def popb32(tag: byte) -> u32 {
		checkTag(tag);
		sp += -(valuerep.slot_size);
		return (sp + valuerep.tag_size).load<u32>();
	}
	def popb64(tag: byte) -> u64 {
		checkTag(tag);
		sp += -(valuerep.slot_size);
		return (sp + valuerep.tag_size).load<u64>();
	}
	def popObject() -> Object {
		if (valuerep.tagged) {
			var got = peekTag();
			match (got) {
				BpTypeCode.ANYREF.code,
				BpTypeCode.FUNCREF.code,
				BpTypeCode.EXTERNREF.code,
				BpTypeCode.REF_NULL.code,
				BpTypeCode.REF.code,
				BpTypeCode.I31REF.code => ;
				_ => fatal(Strings.format1("value stack tag mismatch, expected ref, got %x", got));
			}
		}
		sp += -(valuerep.slot_size);
		return (sp + valuerep.tag_size).load<Object>();
	}
	def popFunction() -> Function {
		var obj = popObject();
		return Function.!(obj);
	}
	def popRtt() -> RttObject {
		var obj = popObject();
		return RttObject.!(obj);
	}
	def checkTag(tag: byte) -> byte {
		if (!valuerep.tagged) return tag;
		var got = peekTag();
		if (got == tag) return tag;
		fatal(Strings.format2("value stack tag mismatch, expected: %x, got %x", tag, got));
		return tag;
	}
	def peekTag() -> byte {
		return (sp + -(valuerep.slot_size)).load<u8>() & '\x7F';
	}
	def pushPair<T>(tag: byte, bits: T) {
		if (valuerep.tagged) sp.store<u8>(tag);
		(sp + valuerep.tag_size).store(bits);
		sp += valuerep.slot_size;
	}
	// GC callback to scan (tagged) values on this stack
	def scan() {
		for (p = mapping.range.start; p < sp; p += valuerep.slot_size) {
			if (p.load<byte>() == BpTypeCode.REF_NULL.code) RiGc.scanRoot(p + valuerep.tag_size);
		}
	}
}

def fatal(msg: string) {
	System.error("X86_64Interpreter", msg);
}

//------------------------------------------------------------------------------------------------
//-- Begin Interpreter Generator
//------------------------------------------------------------------------------------------------

// Internal register configuration for variables live in the interpreter execution context.
def R: X86_64Regs, G = X86_64Regs.GPRs, C: X86_64Conds;
enum IVar(entryParam: int, gpr: X86_64Gpr, frameOffset: int, baseline: bool, mutable: bool) {
	DISPATCH_TABLE	(-1,	R.R14,  -1,	false,  false), // dispatch table
	WASM_FUNC	(1, 	null,	 0,	false,	false),	// WasmFunction object
	MEM0_BASE	(-1,	R.R10,	 8,	true,	false),	// base of memory #0
	VFP		(-1,	R.R11,	16,	false,	false),	// value stack frame pointer
	VSP		(2,	R.RSI,	24,	false,	true),	// value stack stack pointer
	SIDETABLE	(-1,	null,	32,	false,  false), // sidetable
	STP		(-1,	R.RBX,	40,	false,	true),	// sidetable pointer
	CODE		(-1,	null,	48,	false,  false), // code array
	IP		(-1,	R.RAX,	56,	false,	true),	// current instruction pointer
	EIP		(-1,	R.R13,	64,	false,	false),	// end instruction pointer
	FUNC_DECL	(-1,	R.R12,	72,	true,	false),	// FuncDecl
	INSTANCE	(-1,	R.RDI,	80,	true,	false)	// Instance
}
// Shorthand for codegen.
component I {
	def INTERPRETER_GPRS = filterGprs(isTrue);	// allocatable interpreter registers
	def BASELINE_GPRS = filterGprs(IVar.baseline);	// allocatable baseline registers

	def filterGprs(cond: IVar -> bool) -> Array<X86_64Gpr> {
		var gprs = G;
		var used = Array<bool>.new(gprs.length);
		used[R.RSP.regnum] = true;
		for (v in IVar) {
			if (v.gpr != null && cond(v)) used[v.gpr.regnum] = true;
		}
		var v = Vector<X86_64Gpr>.new().grow(gprs.length);
		for (i < gprs.length) {
			if (!used[i]) v.put(gprs[i]);
		}
		return v.extract();
	}
	def isTrue(v: IVar) -> bool {
		return true;
	}

	// Shorthand for individual registers and addresses.
	def ip = R.RAX;
	def eip = R.R13;
	def ip_ptr = ip.indirect();
	def vsp = R.RSI;
	def r0 = I.INTERPRETER_GPRS[0];
	def r1 = I.INTERPRETER_GPRS[1];
	def r2 = I.INTERPRETER_GPRS[2];
	def r3 = I.INTERPRETER_GPRS[3];
	def r4 = I.INTERPRETER_GPRS[4];
	def xmm0 = R.XMM0;
	def xmm1 = R.XMM1;
	def scratch = R.RBP;
}

// Space needed for the machine code of the interpreter
def PAGE_SIZE = 4 * 1024;
def INL_SIZE = 24 * 1024;
def OOL_SIZE = 4 * 1024;
def TOTAL_SIZE = INL_SIZE + OOL_SIZE;
// Statically-allocated buffer (in compiled binary).
def global_buffer = allocGlobalBufferWithMarker(TOTAL_SIZE + PAGE_SIZE);
def allocGlobalBufferWithMarker(size: int) -> Array<byte> {
	var result = Array<byte>.new(size);
	var w = DataWriter.new().reset(result, 0, result.length);
	w.put_b64(long.!(GLOBAL_BUFFER_MARKER));
	return result;
}
def deserializeOrGenerateInterpreterCode() -> X86_64InterpreterCode {
	def r = DataReader.new(global_buffer);

	var start = Pointer.atElement(global_buffer, 0);
	var range = MemoryRange.new(start, start + global_buffer.length);

	var ic = X86_64InterpreterCode.new(range.start, range.end);

	var mask = 4095L;
	var skip = mask & (PAGE_SIZE - ((start - Pointer.NULL) & mask));

	// try deserializing the interpreter code object directly from the global buffer
	if (ic.deserialize(r.at(8))) {
		if (Trace.interpreter) {
			Trace.OUT.put3("Deserialized asm interpreter into (0x%x ... 0x%x), skipping %d bytes",
				(range.start - Pointer.NULL),
				(range.end - Pointer.NULL),
				skip);
			Trace.OUT.outln();
		}

	} else {
		if (Trace.interpreter) {
			Trace.OUT.put3("Generating asm interpreter into (0x%x ... 0x%x), skipping %d bytes",
				(range.start - Pointer.NULL),
				(range.end - Pointer.NULL),
				skip);
			Trace.OUT.outln();
		}
		var w = DataWriter.new().reset(global_buffer, 0, 0);
		w.skipN(int.!(skip));			// align to page
		w.skipN(GLOBAL_BUFFER_HEADER_SIZE);	// skip global header
		X86_64InterpreterGen.new(ic, w).gen(range);
	}

	// Write-protect the executable code for security and debugging
	Mmap.protect(range.start + skip, u64.!(ic.codeEnd - skip), Mmap.PROT_READ | Mmap.PROT_EXEC);
	// Trace results to help in debugging
	if (Trace.interpreter) {
		var s = range.start - Pointer.NULL;
		Trace.OUT
			.put1("\tv3 entry       = 0x%x\n", s + ic.v3EntryOffset)
			.put1("\tfast dispatch  = 0x%x\n", s + ic.fastDispatchTableOffset)
			.put1("\tprobed dispatch= 0x%x\n", s + ic.probedDispatchTableOffset)
			.put1("\toob mem        = 0x%x\n", s + ic.oobMemoryHandlerOffset)
			.put1("\tdivzero        = 0x%x\n", s + ic.divZeroHandlerOffset)
			.put1("\tstack ovflw    = 0x%x\n", s + ic.stackOverflowHandlerOffset)
			.put1("\tcode end       = 0x%x\n", s + ic.codeEnd)
			.outln();
	}
	return ic;
}

// Generates {X86_64InterpreterCode} for X86-64.
class X86_64InterpreterGen(ic: X86_64InterpreterCode, w: DataWriter) {
	def i: X86_64Interpreter;
	def asm = X86_64Assemblers.create64(w);

	def offsets = V3Offsets.new();
	def tuning = X86_64InterpreterTuning.new();
	def valuerep = Target.tagging;

	var oolULeb32Sites = Vector<OutOfLineLEB>.new();
	var firstDispatchOffset: int;
	var dispatchJmpOffset: int = -1;
	var callEntryOffset: int;
	var handlerEndOffset: int;
	var callReentryRef: IcCodeRef;
	var tailCallReentryRef: IcCodeRef;
	var abruptRetRef: IcCodeRef;
	var probedDispatchTableRef: IcCodeRef;
	var typeTagTableOffset: int;
	var useTypeTagTable = false;

	def vsp = I.vsp;
	def vsp_p0t = vsp.indirect();						// VSP[0] tag
	def vsp_p0 = vsp.plus(valuerep.tag_size);				// VSP[0] value
	def vsp_m1 = vsp.plus(valuerep.tag_size - valuerep.slot_size);		// VSP[-1] value
	def vsp_m1u = vsp.plus(valuerep.tag_size - valuerep.slot_size + 4);	// VSP[-1] upper 32 bits
	def vsp_m1t = vsp.plus(-valuerep.slot_size);				// VSP[-1] tag
	def vsp_m2 = vsp.plus(valuerep.tag_size - 2 * valuerep.slot_size);	// VSP[-2] value
	def vsp_m2u = vsp.plus(valuerep.tag_size - 2 * valuerep.slot_size + 4);	// VSP[-2] upper 32 bits
	def vsp_m2t = vsp.plus(-2 * valuerep.slot_size);			// VSP[-2] tag
	def vsp_m3 = vsp.plus(valuerep.tag_size - 3 * valuerep.slot_size);	// VSP[-3] value

	def dispatchTables = Array<(byte, IcCodeRef, IcCodeRef)>.new(Opcodes.code_pages.length + 1);

	new() {
		w.refill = reportOom;
		var p = Patcher.new(w);
		asm.patcher = asm.d.patcher = p;
	}

	def gen(range: MemoryRange) {
		// Reserve space for pointers to interpreter runtime calls
		if (tuning.dispatchEntrySize == 4 && (range.start - Pointer.NULL) > int.max) {
			fatal(Strings.format1("global buffer start address of 0x%x out of 31-bit range", (range.start - Pointer.NULL)));
		}
		if (tuning.useTypeTagTable) {
			// Reserve space for the type tag table and fill it out
			typeTagTableOffset = w.pos;
			w.skipN(256);
			for (t in BpTypeCode) {
				var offset = typeTagTableOffset + t.code;
				w.at(offset).putb(t.code);
				w.at(offset + 0x80).putb(t.code);
			}
			for (t in [BpTypeCode.REF_NULL, BpTypeCode.REF, BpTypeCode.RTT]) { // set upper bit
				var offset = typeTagTableOffset + t.code;
				w.at(offset).putb(t.code | 0x80);
				w.at(offset + 0x80).putb(t.code | 0x80);
			}
			for (t in [BpTypeCode.ABS]) { // TODO: load default value from instance
				var offset = typeTagTableOffset + t.code;
				w.at(offset).putb(BpTypeCode.REF_NULL.code | 0x80);
				w.at(offset + 0x80).putb(BpTypeCode.REF_NULL.code | 0x80);
			}
		}

		// Reserve space for the dispatch tables.
		reserveDispatchTables();
		// Begin code generation
		genInterpreterEntry();
		genOpcodeHandlers();
		handlerEndOffset = w.atEnd().pos;
		// Generate out-of-line code
		genOutOfLineLEBs();
		genTrapHandlers();
		ic.codeEnd = w.atEnd().pos;

		if (Trace.interpreter) {
			var s = range.start - Pointer.NULL;
			Trace.OUT
				.put3("Finished asm interpreter @ (0x%x ... 0x%x), used %d bytes\n",
					s, (range.end - Pointer.NULL), w.pos)
				.put1("\tcall entry     = 0x%x\n", s + callEntryOffset);
			for (t in dispatchTables) Trace.OUT.put2("\tdispatch %x    = 0x%x\n", byte.view(t.0), s + t.1.offset);
			Trace.OUT
				.put1("\tfirst dispatch = 0x%x\n", s + firstDispatchOffset)
				.put1("\thandlers end   = 0x%x\n", s + handlerEndOffset)
				.put1("break *0x%x\n", s + dispatchJmpOffset)
				.outln();
		}
		if (w.pos > TOTAL_SIZE) fatal(Strings.format2("need %d bytes for interpreter code, only allocated %d", w.pos, TOTAL_SIZE));
	}
	def reserveDispatchTables() {
		{ // table #0
			w.align(tuning.dispatchEntrySize);
			var ref = IcCodeRef.new(-1);
			ref.offset = ic.fastDispatchTableOffset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
			dispatchTables[0] = (0, ref, null);
		}
		if (tuning.dispatchTableReg) {
			w.align(tuning.dispatchEntrySize);
			probedDispatchTableRef = IcCodeRef.new(-1);
			probedDispatchTableRef.offset = ic.probedDispatchTableOffset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
		}
		for (i < Opcodes.code_pages.length) {
			var page = Opcodes.code_pages[i];
			var ref = IcCodeRef.new(-1);
			w.align(tuning.dispatchEntrySize);
			ref.offset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
			var ref2: IcCodeRef;

			if (!page.oneByte) {
				ref2 = IcCodeRef.new(-1);
				ref2.offset = w.pos;
				w.skipN(256 * tuning.dispatchEntrySize);
			}

			dispatchTables[i + 1] = (page.prefix, ref, ref2);
		}
	}
	def genInterpreterEntry() {
		var shared_entry = X86_64Label.new();
		var tmp = I.scratch;
		{ // Entrypoint for calls coming from V3
			ic.v3EntryOffset = w.pos;

			// Allocate and initialize interpreter stack frame from incoming V3 params.
			asm.q.sub_r_i(R.RSP, ic.frameSize);

			// Spill VSP (value stack pointer)
			asm.movq_m_r(R.RSP.plus(IVar.VSP.frameOffset), Target.V3_PARAM_GPRS[IVar.VSP.entryParam]);
			// load dispatch table into register
			if (tuning.dispatchTableReg) {
				asm.movq_r_m(IVar.DISPATCH_TABLE.gpr, absPointer(offsets.Interpreter_dispatchTable));
			}
			// move WasmFunction into tmp
			asm.movq_r_r(tmp, Target.V3_PARAM_GPRS[IVar.WASM_FUNC.entryParam]);
			restoreIVar(IVar.VSP);
			asm.jmp_rel_near(shared_entry);
		}

		{ // Re-entry for calls within the interpreter itself
			callReentryRef = IcCodeRef.new(w.pos);
			// Allocate actual stack frame
			asm.q.sub_r_i(R.RSP, ic.frameSize);
			// Spill the (valid) stack pointer
			saveIVar(IVar.VSP);
			// WasmFunction is in r1 for interpreter reentry
			asm.movq_r_r(tmp, I.r1);
		}

		asm.bind(shared_entry);
		// Load wf.instance, wf.decl and spill
		asm.movq_m_r(R.RSP.plus(IVar.WASM_FUNC.frameOffset), tmp);
		asm.movq_r_m(IVar.INSTANCE.gpr, tmp.plus(offsets.WasmFunction_instance));
		saveIVar(IVar.INSTANCE);
		asm.movq_r_m(IVar.FUNC_DECL.gpr, tmp.plus(offsets.WasmFunction_decl));
		saveIVar(IVar.FUNC_DECL);

		// Compute VFP = VSP - func.sig.params.length * SLOT_SIZE
		asm.movq_r_m(tmp, IVar.FUNC_DECL.gpr.plus(offsets.FuncDecl_sig));
		asm.movq_r_m(tmp, tmp.plus(offsets.SigDecl_params));
		asm.movd_r_m(tmp, tmp.plus(offsets.Array_length));
		asm.q.shl_r_i(tmp, valuerep.slot_size_log);
		asm.movq_r_r(IVar.VFP.gpr, IVar.VSP.gpr);
		asm.q.sub_r_r(IVar.VFP.gpr, tmp);
		saveIVar(IVar.VFP);

		tailCallReentryRef = IcCodeRef.new(w.pos);
		// Load &func.code.code[0] into IP
		var codeReg = I.r0;
		asm.movq_r_m(codeReg, IVar.FUNC_DECL.gpr.plus(offsets.FuncDecl_code));
		asm.movq_r_m(tmp, codeReg.plus(offsets.Code_code));
		asm.movq_m_r(R.RSP.plus(IVar.CODE.frameOffset), tmp); // save CODE
		asm.lea(IVar.IP.gpr, tmp.plus(offsets.Array_contents));
		saveIVar(IVar.IP);
		// Load IP + code.length into EIP
		asm.movd_r_m(IVar.EIP.gpr, tmp.plus(offsets.Array_length));
		asm.q.add_r_r(IVar.EIP.gpr, IVar.IP.gpr);
		saveIVar(IVar.EIP);
		// Load &func.code.sidetable[0] into STP
		asm.movq_r_m(tmp, codeReg.plus(offsets.Code_sidetable));
		asm.movq_m_r(R.RSP.plus(IVar.SIDETABLE.frameOffset), tmp); // save SIDETABLE
		asm.lea(IVar.STP.gpr, tmp.plus(offsets.Array_contents));
		saveIVar(IVar.STP);

		// Load instance.memories[0].start into MEM0_BASE
		var mem0 = IVar.MEM0_BASE.gpr;
		asm.movq_r_m(mem0, IVar.INSTANCE.gpr.plus(offsets.Instance_memories));
		var no_mem = X86_64Label.new();
		asm.movd_r_m(I.r0, mem0.plus(offsets.Array_length)); // XXX: always have a memories[0].start to avoid a branch?
		asm.d.cmp_r_i(I.r0, 0);
		asm.jc_rel_near(C.Z, no_mem);
		asm.movq_r_m(mem0, mem0.plus(offsets.Array_contents));
		asm.movq_r_m(mem0, mem0.plus(offsets.X86_64Memory_start));
		asm.bind(no_mem);
		saveIVar(IVar.MEM0_BASE);

		callEntryOffset = w.pos;
		// Decode locals and initialize them. (XXX: special-case 0 locals)
		var countGpr = I.r0;
		genReadUleb32(countGpr);
		var start = X86_64Label.new(), done = X86_64Label.new();
		// gen: if (count != 0) do
		asm.d.cmp_r_i(countGpr, 0);
		asm.jc_rel_near(C.Z, done);
		asm.bind(start);
		// gen: var num = read_uleb32()
		var numGpr = I.r1;
		genReadUleb32(numGpr);
		// gen: var type = read_type();
		var type_leb_done = X86_64Label.new(), type_done = X86_64Label.new();
		var typeGpr = I.r2;
		asm.d.movbzx_r_m(typeGpr, I.ip_ptr);	// load first byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.d.test_r_i(typeGpr, 0x80);		// test most-significant bit
		asm.jc_rel_near(C.Z, type_leb_done);	// more type LEB bytes?
		genSkipLeb();				// only first byte matters
		asm.bind(type_leb_done);
		// gen: if((type = typeTags[type]) < 0) skip_leb()
		if (tuning.useTypeTagTable) {
			// TODO: AbstractT requires a load of the default value from the instance
			asm.q.lea(I.scratch, IcCodeRef.new(typeTagTableOffset));   // materialize type tag table address
			asm.d.movbsx_r_m(typeGpr, typeGpr.plusR(I.scratch, 1, 0)); // load from type tag table
			asm.jc_rel_near(C.NC, type_done);	// check upper bit of entry
			genSkipLeb();				// skip LEB if upper bit of entry set
		}
		asm.bind(type_done);

		// gen: if(num != 0) do
		var start2 = X86_64Label.new(), done2 = X86_64Label.new();
		asm.d.cmp_r_i(numGpr, 0);
		asm.jc_rel_near(C.Z, done2);
		asm.bind(start2);
		genTagPushR(typeGpr);			// *(sp) = type
		asm.movq_m_i(vsp_p0, 0);		// *(sp + 8) = 0
		asm.add_r_i(vsp, valuerep.slot_size);		// sp += 16
		// gen: while (--num != 0)
		asm.d.dec_r(numGpr);
		asm.jc_rel_near(C.NZ, start2);

		// gen: while (--count != 0)
		asm.d.dec_r(countGpr);
		asm.jc_rel_near(C.NZ, start);
		asm.bind(done);

		// execute first instruction
		firstDispatchOffset = w.pos;
		genDispatch(I.ip_ptr, if(!tuning.dispatchTableReg, dispatchTables[0].1), true);
	}

	// Generate all the opcode handlers.
	def genOpcodeHandlers() {
		// Generate the default handler and initialize dispatch tables
		var pos = w.atEnd().pos;
		genTrap(TrapReason.INVALID_OPCODE);
		for (t in dispatchTables) {
			var ref = t.1;
			for (i < 256) writeDispatchEntry(ref, i, pos);
		}

		// Generate the secondary dispatch tables and point main table at them
		var ref0 = dispatchTables[0].1;
		for (t in dispatchTables) {
			if (t.0 == 0) continue; // main dispatch table
			var pos = w.atEnd().pos;
			genDispatch(I.ip_ptr, t.1, true);
			writeDispatchEntry(ref0, t.0, pos);
		}

		// Generate extended LEB landing pads for secondary dispatch tables
		for (i = 1; i < dispatchTables.length; i++) {
			var t = dispatchTables[i];
			var pos = w.pos;
			if (t.2 != null) {
				// TODO: some code in this page are in the upper 128; read extended LEB
				genDispatch(null, t.2, false);
			} else {
				// all codes in this page are in the lower 128; just skip extended LEB
				genSkipLeb();
				asm.d.and_r_i(I.r0, 0x7F);
				genDispatch(null, t.1, false);
			}
			for (i = 128; i < 256; i++) {
				writeDispatchEntry(t.1, i, pos);
			}
		}

		genConsts();
		genControlFlow();
		genLocals();
		genCallsAndRet();
		genLoadsAndStores();
		genCompares();
		genI32Arith();
		genI64Arith();
		genExtensions();
		genF32Arith();
		genF64Arith();
		genFloatCmps();
		genFloatMinAndMax();
		genFloatTruncs();
		genFloatConversions();
		genRuntimeCallOps();
		genMisc();
	}
	def writeDispatchEntry(ref: IcCodeRef, opcode: int, offset: int) {
		match (tuning.dispatchEntrySize) {
			2 => w.at(ref.offset + 2 * opcode).put_b16(offset - ref.offset);
			4 => w.at(ref.offset + 4 * opcode).put_b32(int.!((ic.start + offset) - Pointer.NULL));
			8 => w.at(ref.offset + 8 * opcode).put_b64((ic.start + offset) - Pointer.NULL);
		}
		w.atEnd();
	}
	def genConsts() {
		bindHandler(Opcode.I32_CONST); {
			genReadSleb32_inline(I.r1);
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsp_p0, I.r1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_CONST); {
			genReadSleb64_inline(I.r1);
			genTagPush(BpTypeCode.I64.code);
			asm.movq_m_r(vsp_p0, I.r1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_CONST); {
			asm.movd_r_m(I.r0, I.ip_ptr);
			asm.add_r_i(I.ip, 4);
			genTagPush(BpTypeCode.F32.code);
			asm.movq_m_r(vsp_p0, I.r0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_CONST); {
			asm.movq_r_m(I.r0, I.ip_ptr);
			asm.add_r_i(I.ip, 8);
			genTagPush(BpTypeCode.F64.code);
			asm.movq_m_r(vsp_p0, I.r0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genControlFlow() {
		// NOP: just goes directly back to the dispatch loop
		patchDispatchTable(Opcode.NOP, firstDispatchOffset);

		// UNREACHABLE: abrupt return
		bindHandler(Opcode.UNREACHABLE);
		genTrap(TrapReason.UNREACHABLE);

		// BLOCK, LOOP, and TRY are nops except skipping the LEB
		bindHandler(Opcode.BLOCK);
		bindHandler(Opcode.LOOP);
		bindHandler(Opcode.TRY);
		genSkipLeb();
		endHandler();

		var ctl_fallthru = X86_64Label.new();
		var ctl_xfer = X86_64Label.new();
		var ctl_xfer_nostack = X86_64Label.new();

		// IF: check condition and either fall thru to next bytecode or ctl xfer (without stack copying)
		bindHandler(Opcode.IF);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.d.cmp_m_i(vsp_p0, 0);
		asm.jc_rel_near(C.Z, ctl_xfer_nostack);
		asm.bind(ctl_fallthru);
		genSkipLeb();
		asm.add_r_i(IVar.STP.gpr, offsets.STP_entry_size);
		endHandler();

		// BR_IF: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_IF);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.d.cmp_m_i(vsp_p0, 0);
		asm.jc_rel_near(C.Z, ctl_fallthru);
		// fallthru to BR

		// BR: unconditional ctl xfer with stack copying
		bindHandler(Opcode.BR);
		asm.bind(ctl_xfer);
		var popcount = I.r0;
		var valcount = I.r1;
		// if popcount > 0
		asm.movd_r_m(popcount, IVar.STP.gpr.plus(offsets.STP_popcount));
		asm.d.cmp_r_i(popcount, 0);
		asm.jc_rel_near(C.Z, ctl_xfer_nostack);
		// load valcount
		asm.movd_r_m(valcount, IVar.STP.gpr.plus(offsets.STP_valcount));
		// popcount = popcount * SLOT_SIZE
		asm.d.shl_r_i(popcount, valuerep.slot_size_log);
		// vsp -= valcount + popcount (XXX: save an instruction here?)
		asm.q.sub_r_r(vsp, popcount);
		asm.movd_r_r(I.scratch, valcount);
		asm.d.shl_r_i(I.scratch, valuerep.slot_size_log);
		asm.q.sub_r_r(vsp, I.scratch);
		// do { [vsp] = [vsp + popcount]; vsp++; valcount--; } while (valcount != 0)
		var loop = X86_64Label.new();
		asm.bind(loop);
		genCopySlot(vsp.plus(0), vsp.plusR(popcount, 1, 0));
		asm.q.add_r_i(vsp, valuerep.slot_size);
		asm.d.dec_r(valcount);
		asm.jc_rel_near(C.G, loop);

		// ELSE: unconditional ctl xfer without stack copying
		bindHandler(Opcode.ELSE);
		asm.bind(ctl_xfer_nostack);
		asm.movwsx_r_m(I.r0, IVar.STP.gpr.plus(offsets.STP_pc_delta)); // TODO: 4 bytes
		asm.q.lea(IVar.IP.gpr, IVar.IP.gpr.plusR(I.r0, 1, -1)); // adjust ip
		asm.movwsx_r_m(I.r1, IVar.STP.gpr.plus(offsets.STP_xip_delta)); // TODO: 4 bytes
		asm.q.lea(IVar.STP.gpr, IVar.STP.gpr.plusR(I.r1, 4, 0)); // adjust xip XXX: preshift?
		endHandler();

		// BR_TABLE: adjust STP based on input value and then ctl xfer with stack copying
		bindHandler(Opcode.BR_TABLE);
		var max = I.r0, key = I.r1;
		asm.movd_r_m(max, IVar.STP.gpr.plus(offsets.STP_pc_delta));
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.movd_r_m(key, vsp_p0);
		asm.d.cmp_r_r(key, max);
		var ok = X86_64Label.new();
		asm.jc_rel_near(C.NC, ok);
		asm.d.inc_r(key);
		asm.movd_r_r(max, key);
		asm.bind(ok);
		asm.q.add_r_r(IVar.IP.gpr, max);
		asm.shl_r_i(max, offsets.STP_entry_size_log);
		asm.q.add_r_r(IVar.STP.gpr, max);
		asm.jmp_rel_near(ctl_xfer);

		bindHandler(Opcode.SELECT); {
			var label = X86_64Label.new();
			asm.d.cmp_m_i(vsp_m1, 0);
			asm.jc_rel_near(C.NZ, label);
			// false case; copy false value down
			asm.movq_r_m(I.r0, vsp_m2);
			asm.movq_m_r(vsp_m3, I.r0);
			// true case, nothing to do
			asm.bind(label);
			asm.sub_r_i(vsp, 2 * valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.SELECT_T); {
			genReadUleb32(I.r0); // load # values
			var skip = X86_64Label.new();
			asm.movd_r_r(I.r1, I.r0);
			asm.bind(skip);  // skip value types
			genSkipLeb();
			asm.dec_r(I.r1);
			asm.jc_rel_near(C.NZ, skip);

			asm.d.shl_r_i(I.r0, valuerep.slot_size_log);
			asm.movd_r_m(I.r1, vsp_m1);
			asm.sub_r_r(vsp, I.r0);
			asm.sub_r_i(vsp, valuerep.slot_size); // XXX: combine with above using lea
			asm.d.cmp_r_i(I.r1, 0);
			var label = X86_64Label.new();
			asm.jc_rel_near(C.NZ, label);
			// false case; copy false values down
			asm.movq_r_r(I.r1, vsp);
			asm.q.sub_r_r(I.r1, I.r0);
			var copy = X86_64Label.new();
			asm.bind(copy);
			asm.movq_r_m(I.r2, vsp.plusR(I.r0, 1, - Pointer.SIZE));
			asm.movq_m_r(I.r1.plusR(I.r0, 1, - Pointer.SIZE), I.r2);
			asm.d.sub_r_i(I.r0, valuerep.slot_size);
			asm.jc_rel_near(C.NZ, copy);
			// true case, nothing to do
			asm.bind(label);
			endHandler();
		}
	}
	def genLocals() {
		bindHandler(Opcode.DROP);
		asm.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.LOCAL_GET);
		genReadUleb32(I.r0);
		asm.d.shl_r_i(I.r0, valuerep.slot_size_log);
		genCopySlot(vsp.indirect(), IVar.VFP.gpr.plusR(I.r0, 1, 0));
		asm.add_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.LOCAL_SET);
		genReadUleb32(I.r0);
		asm.d.shl_r_i(I.r0, valuerep.slot_size_log);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.movq_r_m(I.r1, vsp_p0);
		asm.movq_m_r(IVar.VFP.gpr.plusR(I.r0, 1, valuerep.tag_size), I.r1);
		endHandler();

		bindHandler(Opcode.LOCAL_TEE);
		genReadUleb32(I.r0);
		asm.d.shl_r_i(I.r0, valuerep.slot_size_log);
		asm.movq_r_m(I.r1, vsp_m1);
		asm.movq_m_r(IVar.VFP.gpr.plusR(I.r0, 1, valuerep.tag_size), I.r1);
		endHandler();
	}
	def genCallsAndRet() {
		bindHandler(Opcode.END);
		asm.q.cmp_r_r(I.ip, I.eip);
		// XXX: END: jump over inlined dispatch?
		asm.jc_rel(C.L, firstDispatchOffset - w.pos); // jump to dispatch (loop)
		// end falls through to return bytecode

		var callFunction = X86_64Label.new();
		bindHandler(Opcode.RETURN); {
			// Copy return values from stack to overwrite locals
			var cnt = I.r0;
			asm.movq_r_m(cnt, IVar.FUNC_DECL.gpr.plus(offsets.FuncDecl_sig));
			asm.movq_r_m(cnt, cnt.plus(offsets.SigDecl_results));
			asm.movd_r_m(cnt, cnt.plus(offsets.Array_length));
			genCopyStackValsToVfp(cnt, I.r1);
			// Deallocate interpreter frame and return to calling code.
			asm.q.add_r_i(R.RSP, ic.frameSize);
			asm.movd_r_i(Target.V3_RET_GPRS[0], ExecState.FINISHED.tag);
			asm.ret();

			bindHandler(Opcode.CALL);
			// XXX: save ip prior to call for stack traces?
			genReadUleb32(I.r1);

			asm.movq_r_m(I.r0, IVar.INSTANCE.gpr.plus(offsets.Instance_functions));
			asm.movq_r_m(I.r1, I.r0.plusR(I.r1, offsets.REF_SIZE, offsets.Array_contents));

			// call_indirect jumps here
			asm.bind(callFunction);
			saveCallerIVars();
			var call_host = X86_64Label.new();
			asm.d.cmp_m_i(I.r1.plus(0), offsets.WasmFunction_typeId);
			asm.jc_rel_near(C.NZ, call_host);

			// WasmFunction: call into interpreter reentry
			asm.callr_addr(callReentryRef);
			genExecStateCheck();
			restoreCallerIVars();
			genDispatchOrJumpToDispatch();

			// HostFunction: call into interpreter runtimeCall to enter into V3 code
			asm.bind(call_host);
			saveIVar(IVar.VSP);
			callRuntime(refRuntimeCall(this.i.runtime_callHost), [I.r1], true);
			restoreCallerIVars();
			genDispatchOrJumpToDispatch();
		}

		var trap_oob = X86_64Label.new(), trap_sig_mismatch = X86_64Label.new();
		bindHandler(Opcode.CALL_INDIRECT); {
			// XXX: save ip prior to call for stack traces?
			var sig_index = I.r1, table_index = I.r2, func_index = I.r0;
			genReadUleb32(sig_index);
			genReadUleb32(table_index);

			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movd_r_m(func_index, vsp_p0);

			var tmp = I.r3;
			// load instance.sig_ids[sig_index] into sig_index
			asm.movq_r_m(tmp, IVar.INSTANCE.gpr.plus(offsets.Instance_sig_ids));
			asm.movd_r_m(sig_index, tmp.plusR(sig_index, offsets.INT_SIZE, offsets.Array_contents));
			// Bounds-check table.ids[func_index]
			asm.movq_r_m(tmp, IVar.INSTANCE.gpr.plus(offsets.Instance_tables));
			var table = table_index;
			asm.movq_r_m(table, tmp.plusR(table_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(tmp, table.plus(offsets.Table_ids));
			asm.d.cmp_r_m(func_index, tmp.plus(offsets.Array_length));
			asm.jc_rel_near(C.NC, trap_oob);
			// Check table.ids[func_index] == sig_index
			asm.d.cmp_r_m(sig_index, tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents));
			asm.jc_rel_near(C.NZ, trap_sig_mismatch);
			// Load table.funcs[func_index] into r1 and jump to calling sequence
			asm.movq_r_m(tmp, table.plus(offsets.Table_funcs));
			asm.movq_r_m(I.r1, tmp.plusR(func_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.jmp_rel_near(callFunction);
			// XXX: move traps for call indirect out-of-line
			asm.bind(trap_oob);
			genTrap(TrapReason.FUNC_INVALID);
			asm.bind(trap_sig_mismatch);
			asm.d.cmp_m_i(tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents), 0);
			asm.jc_rel_near(C.S, trap_oob); // < 0 implies invalid function, not function sig mismatch
			genTrap(TrapReason.FUNC_SIG_MISMATCH);
		}

		var tailCallFunction = X86_64Label.new();
		bindHandler(Opcode.RETURN_CALL); {
			// XXX: save ip prior to call for stack traces?
			genReadUleb32(I.r1);

			asm.movq_r_m(I.r0, IVar.INSTANCE.gpr.plus(offsets.Instance_functions));
			var target_func = I.r1;
			asm.movq_r_m(target_func, I.r0.plusR(I.r1, offsets.REF_SIZE, offsets.Array_contents));

			// return_tail_call jumps here
			asm.bind(tailCallFunction);
			// Overwrite current locals with outgoing arguments
			var cnt = I.r0;
			asm.movq_r_m(cnt, target_func.plus(offsets.Function_sig));
			asm.movq_r_m(cnt, cnt.plus(offsets.SigDecl_params));
			asm.movd_r_m(cnt, cnt.plus(offsets.Array_length));
			genCopyStackValsToVfp(cnt, I.r2);

			// Check if the target function is a WasmFunction or HostFunction
			var tail_call_host = X86_64Label.new();
			asm.d.cmp_m_i(target_func.plus(0), offsets.WasmFunction_typeId);
			asm.jc_rel_near(C.NZ, tail_call_host);

			// WasmFunction: jump into interpreter reentry
			asm.q.lea(IVar.VSP.gpr, IVar.VFP.gpr.plusR(cnt, 1, 0)); // set VSP properly
			asm.movq_r_m(IVar.INSTANCE.gpr, target_func.plus(offsets.WasmFunction_instance));
			saveIVar(IVar.INSTANCE);
			asm.movq_r_m(IVar.FUNC_DECL.gpr, target_func.plus(offsets.WasmFunction_decl));
			saveIVar(IVar.FUNC_DECL);
			asm.jmp_rel_addr(tailCallReentryRef);

			// HostFunction: jump into interpreter runtimeCall to enter into V3 code
			asm.bind(tail_call_host);

			// Custom code to tail-call the runtime
			asm.movq_r_r(I.r3, IVar.VSP.gpr); // save VSP from being overwritten
			var abs = refRuntimeCall(this.i.runtime_callHost);
			var regs = [target_func];
			// Generate parallel moves from args into param gprs; assume each src register used only once
			var dst = Array<X86_64Gpr>.new(G.length);
			for (i < regs.length) {
				var sreg = regs[i];
				var dreg = Target.V3_PARAM_GPRS[i + 1];
				if (sreg != dreg) dst[sreg.regnum] = dreg;
			}
			var stk = Array<i8>.new(G.length);
			for (i < dst.length) orderMoves(dst, stk, i);
			// load interpreter into first arg register
			var interp = Target.V3_PARAM_GPRS[0];
			// save a copy of VSP into interpreter.valueStack.sp
			asm.movq_r_m(I.scratch, absPointer(offsets.Interpreter_valueStack));
			asm.movq_m_r(I.scratch.plus(offsets.ValueStack_sp), I.r3);
			asm.q.add_r_i(R.RSP, ic.frameSize); // deallocate interpreter frame
			asm.jmp_rel(int.view(u32.!(abs - (ic.start + w.pos)))); // tail-call into runtime
		}

		bindHandler(Opcode.RETURN_CALL_INDIRECT); {
			// XXX: save ip prior to call for stack traces?
			var sig_index = I.r1, table_index = I.r2, func_index = I.r0;
			genReadUleb32(sig_index);
			genReadUleb32(table_index);

			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movd_r_m(func_index, vsp_p0);

			var tmp = I.r3;
			// load instance.sig_ids[sig_index] into sig_index
			asm.movq_r_m(tmp, IVar.INSTANCE.gpr.plus(offsets.Instance_sig_ids));
			asm.movd_r_m(sig_index, tmp.plusR(sig_index, offsets.INT_SIZE, offsets.Array_contents));
			// Bounds-check table.ids[func_index]
			asm.movq_r_m(tmp, IVar.INSTANCE.gpr.plus(offsets.Instance_tables));
			var table = table_index;
			asm.movq_r_m(table, tmp.plusR(table_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(tmp, table.plus(offsets.Table_ids));
			asm.d.cmp_r_m(func_index, tmp.plus(offsets.Array_length));
			asm.jc_rel_near(C.NC, trap_oob);
			// Check table.ids[func_index] == sig_index
			asm.d.cmp_r_m(sig_index, tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents));
			asm.jc_rel_near(C.NZ, trap_sig_mismatch);
			// Load table.funcs[func_index] into r1 and jump to calling sequence
			asm.movq_r_m(tmp, table.plus(offsets.Table_funcs));
			asm.movq_r_m(I.r1, tmp.plusR(func_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.jmp_rel_near(tailCallFunction);
		}
	}
	def genCopyStackValsToVfp(cnt: X86_64Gpr, i: X86_64Gpr) {
		var done = X86_64Label.new();
		// Copy argument(s) from VSP to VFP.
		asm.cmp_r_i(cnt, 0);
		asm.jc_rel_near(C.Z, done);
		asm.movd_r_i(i, 0);
		asm.d.shl_r_i(cnt, valuerep.slot_size_log);
		asm.q.sub_r_r(IVar.VSP.gpr, cnt);
		var loop = X86_64Label.new();
		// while (i < cnt)
		asm.bind(loop);
		genCopySlot(IVar.VFP.gpr.plusR(i, 1, 0), IVar.VSP.gpr.plusR(i, 1, 0));
		asm.q.add_r_i(i, valuerep.slot_size);
		asm.q.cmp_r_r(i, cnt);
		asm.jc_rel_near(C.L, loop);
		asm.bind(done);
		// set VSP properly
		asm.q.lea(IVar.VSP.gpr, IVar.VFP.gpr.plusR(cnt, 1, 0));
	}
	def genLoadsAndStores() {
		genLoad(Opcode.I32_LOAD, BpTypeCode.I32.code, asm.movd_r_m);
		genLoad(Opcode.I64_LOAD, BpTypeCode.I64.code, asm.movq_r_m);
		genLoad(Opcode.F32_LOAD, BpTypeCode.F32.code, asm.movd_r_m);
		genLoad(Opcode.F64_LOAD, BpTypeCode.F64.code, asm.movq_r_m);
		genLoad(Opcode.I32_LOAD8_S, BpTypeCode.I32.code, asm.movbsx_r_m);
		genLoad(Opcode.I32_LOAD8_U, BpTypeCode.I32.code, asm.movbzx_r_m);
		genLoad(Opcode.I32_LOAD16_S, BpTypeCode.I32.code, asm.movwsx_r_m);
		genLoad(Opcode.I32_LOAD16_U, BpTypeCode.I32.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_LOAD8_S, BpTypeCode.I64.code, asm.movbsx_r_m);
		genLoad(Opcode.I64_LOAD8_U, BpTypeCode.I64.code, asm.movbzx_r_m);
		genLoad(Opcode.I64_LOAD16_S, BpTypeCode.I64.code, asm.movwsx_r_m);
		genLoad(Opcode.I64_LOAD16_U, BpTypeCode.I64.code, asm.movwzx_r_m);
		bindHandler(Opcode.I64_LOAD32_S); {
			asm.q.inc_r(I.ip); 				// skip flags byte
			genReadUleb32(I.r0);				// decode offset
			asm.movd_r_m(I.r1, vsp_m1);			// read index
			asm.q.add_r_r(I.r0, I.r1);			// add index + offset
			asm.movd_r_m(I.r1, IVar.MEM0_BASE.gpr.plusR(I.r0, 1, 0));
			asm.q.shl_r_i(I.r1, 32); 			// special sign-extension necessary
			asm.q.sar_r_i(I.r1, 32);
			genTagUpdate(BpTypeCode.I64.code);
			asm.movq_m_r(vsp_m1, I.r1);
			endHandler();
		}
		genLoad(Opcode.I64_LOAD32_U, BpTypeCode.I64.code, asm.movd_r_m);

		bindHandler(Opcode.I32_STORE);
		bindHandler(Opcode.F32_STORE);
		bindHandler(Opcode.I64_STORE32);
		genStore(asm.movd_m_r);

		bindHandler(Opcode.I64_STORE);
		bindHandler(Opcode.F64_STORE);
		genStore(asm.movq_m_r);

		bindHandler(Opcode.I32_STORE8);
		bindHandler(Opcode.I64_STORE8);
		genStore(asm.movb_m_r);

		bindHandler(Opcode.I32_STORE16);
		bindHandler(Opcode.I64_STORE16);
		genStore(asm.movw_m_r);
	}
	def genCompares() {
		// 32-bit integer compares
		for (t in [
			(Opcode.I32_EQ, C.Z),
			(Opcode.I32_NE, C.NZ),
			(Opcode.I32_LT_S, C.L),
			(Opcode.I32_LT_U, C.C),
			(Opcode.I32_GT_S, C.G),
			(Opcode.I32_GT_U, C.A),
			(Opcode.I32_LE_S, C.LE),
			(Opcode.I32_LE_U, C.NA),
			(Opcode.I32_GE_S, C.GE),
			(Opcode.I32_GE_U, C.NC)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(I.r0, vsp_m1);
			asm.d.cmp_m_r(vsp_m2, I.r0);
			asm.set_r(t.1, I.r0);
			asm.movbzx_r_r(I.r0, I.r0);
			asm.movd_m_r(vsp_m2, I.r0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		// 64-bit integer compares
		bindHandler(Opcode.REF_EQ); // share handler with I64_EQ
		for (t in [
			(Opcode.I64_EQ, C.Z),
			(Opcode.I64_NE, C.NZ),
			(Opcode.I64_LT_S, C.L),
			(Opcode.I64_LT_U, C.C),
			(Opcode.I64_GT_S, C.G),
			(Opcode.I64_GT_U, C.A),
			(Opcode.I64_LE_S, C.LE),
			(Opcode.I64_LE_U, C.NA),
			(Opcode.I64_GE_S, C.GE),
			(Opcode.I64_GE_U, C.NC)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(I.r0, vsp_m1);
			asm.q.cmp_m_r(vsp_m2, I.r0);
			asm.set_r(t.1, I.r0);
			asm.movbzx_r_r(I.r0, I.r0);
			asm.movq_m_r(vsp_m2, I.r0);
			if (valuerep.tagged) asm.movq_m_i(vsp_m2t, BpTypeCode.I32.code);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}

	}
	def genI32Arith() {
		bindHandler(Opcode.I32_EQZ); {
			asm.d.test_m_i(vsp_m1, -1);
			asm.set_r(C.Z, I.r0);
			asm.movbzx_r_r(I.r0, I.r0);
			asm.movd_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I32_CLZ); {
			asm.movd_r_i(I.r1, -1);
			asm.d.bsr_r_m(I.r0, vsp_m1);
			asm.d.cmov_r(C.Z, I.r0, I.r1);
			asm.movd_r_i(I.r1, 31);
			asm.d.sub_r_r(I.r1, I.r0);
			asm.movd_m_r(vsp_m1, I.r1);
			endHandler();
		}
		bindHandler(Opcode.I32_CTZ); {
			asm.d.bsf_r_m(I.r0, vsp_m1);
			asm.movd_r_i(I.r1, 32);
			asm.d.cmov_r(C.Z, I.r0, I.r1);
			asm.movd_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I32_POPCNT); {
			asm.d.popcnt_r_m(I.r0, vsp_m1);
			asm.movd_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I32_MUL); {
			asm.movd_r_m(I.r0, vsp_m1);
			asm.d.imul_r_m(I.r0, vsp_m2);
			asm.movd_m_r(vsp_m2, I.r0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I32_DIV_S); {
			var div = X86_64Label.new(), neg = X86_64Label.new(), done = X86_64Label.new();
			asm.movd_r_m(I.r0, vsp_m1);
			asm.d.cmp_r_i(I.r0, -1);
			asm.jc_rel_near(C.NZ, div);
			asm.movd_r_m(I.r0, vsp_m2);
			asm.d.cmp_r_i(I.r0, 0x80000000);
			asm.jc_rel_near(C.NZ, neg);
			genTrap(TrapReason.DIV_UNREPRESENTABLE);
			asm.bind(neg);
			asm.d.neg_m(vsp_m2);
			asm.jmp_rel_near(done);
			asm.bind(div);
			saveReg(R.RAX);
			saveReg(R.RDX);
			asm.movd_r_m(R.RAX, vsp_m2);
			asm.d.cdq();
			asm.d.idiv_r(I.r0);
			asm.movd_m_r(vsp_m2, R.RAX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.bind(done);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I32_DIV_U); {
			asm.movd_r_m(I.r0, vsp_m1);
			saveReg(R.RAX);
			saveReg(R.RDX);
			asm.movd_r_m(R.RAX, vsp_m2);
			asm.movd_r_i(R.RDX, 0);
			asm.d.div_r(I.r0);
			asm.movd_m_r(vsp_m2, R.RAX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I32_REM_S); {
			var div = X86_64Label.new(), done = X86_64Label.new();
			asm.movd_r_m(I.r0, vsp_m1);
			asm.d.cmp_r_i(I.r0, -1);
			asm.jc_rel_near(C.NZ, div);
			asm.movd_m_i(vsp_m2, 0);
			asm.jmp_rel_near(done);
			asm.bind(div);
			saveReg(R.RAX);
			saveReg(R.RDX);
			asm.movd_r_m(R.RAX, vsp_m2);
			asm.d.cdq();
			asm.d.idiv_r(I.r0);
			asm.movd_m_r(vsp_m2, R.RDX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.bind(done);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I32_REM_U); {
			asm.movd_r_m(I.r0, vsp_m1);
			saveReg(R.RAX);
			saveReg(R.RDX);
			asm.movd_r_m(R.RAX, vsp_m2);
			asm.movd_r_i(R.RDX, 0);
			asm.d.div_r(I.r0);
			asm.movd_m_r(vsp_m2, R.RDX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I32_ADD, asm.d.add_m_r),
			(Opcode.I32_SUB, asm.d.sub_m_r),
			(Opcode.I32_AND, asm.d.and_m_r),
			(Opcode.I32_OR, asm.d.or_m_r),
			(Opcode.I32_XOR, asm.d.xor_m_r)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(I.r0, vsp_m1);
			t.1(vsp_m2, I.r0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I32_SHL, asm.d.shl_m_cl),
			(Opcode.I32_SHR_S, asm.d.sar_m_cl),
			(Opcode.I32_SHR_U, asm.d.shr_m_cl),
			(Opcode.I32_ROTL, asm.d.rol_m_cl),
			(Opcode.I32_ROTR, asm.d.ror_m_cl)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(I.r0, vsp_m1);
			t.1(vsp_m2);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genI64Arith() {
		bindHandler(Opcode.I64_EQZ); {
			asm.q.test_m_i(vsp_m1, -1);
			asm.set_r(C.Z, I.r0);
			asm.movbzx_r_r(I.r0, I.r0);
			asm.movd_m_r(vsp_m1, I.r0);
			if (valuerep.tagged) asm.movd_m_i(vsp_m1t, BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I64_CLZ); {
			asm.movq_r_i(I.r1, -1);
			asm.q.bsr_r_m(I.r0, vsp_m1);
			asm.q.cmov_r(C.Z, I.r0, I.r1);
			asm.movd_r_i(I.r1, 63);
			asm.q.sub_r_r(I.r1, I.r0);
			asm.movq_m_r(vsp_m1, I.r1);
			endHandler();
		}
		bindHandler(Opcode.I64_CTZ); {
			asm.q.bsf_r_m(I.r0, vsp_m1);
			asm.movd_r_i(I.r1, 64);
			asm.q.cmov_r(C.Z, I.r0, I.r1);
			asm.movq_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I64_POPCNT); {
			asm.q.popcnt_r_m(I.r0, vsp_m1);
			asm.movq_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I64_MUL); {
			asm.movq_r_m(I.r0, vsp_m1);
			asm.q.imul_r_m(I.r0, vsp_m2);
			asm.movq_m_r(vsp_m2, I.r0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_DIV_S); {
			var div = X86_64Label.new(), neg = X86_64Label.new(), done = X86_64Label.new();
			asm.movq_r_m(I.r0, vsp_m1);
			asm.cmp_r_i(I.r0, -1);
			asm.jc_rel_near(C.NZ, div);
			asm.movq_r_m(I.r0, vsp_m2);
			asm.movq_r_i(I.r1, 0x80);
			asm.shl_r_i(I.r1, 56);
			asm.cmp_r_r(I.r0, I.r1);
			asm.jc_rel_near(C.NZ, neg);
			genTrap(TrapReason.DIV_UNREPRESENTABLE);
			asm.bind(neg);
			asm.neg_m(vsp_m2);
			asm.jmp_rel_near(done);
			asm.bind(div);
			saveReg(R.RAX);
			saveReg(R.RDX);
			asm.movq_r_m(R.RAX, vsp_m2);
			asm.cqo();
			asm.idiv_r(I.r0);
			asm.movq_m_r(vsp_m2, R.RAX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.bind(done);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_DIV_U); {
			asm.movq_r_m(I.r0, vsp_m1);
			saveReg(R.RAX);
			saveReg(R.RDX);
			asm.movq_r_m(R.RAX, vsp_m2);
			asm.movd_r_i(R.RDX, 0);
			asm.div_r(I.r0);
			asm.movq_m_r(vsp_m2, R.RAX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_REM_S); {
			var div = X86_64Label.new(), done = X86_64Label.new();
			asm.movq_r_m(I.r0, vsp_m1);
			asm.cmp_r_i(I.r0, -1);
			asm.jc_rel_near(C.NZ, div);
			asm.movq_m_i(vsp_m2, 0);
			asm.jmp_rel_near(done);
			asm.bind(div);
			saveReg(R.RAX);
			saveReg(R.RDX);
			asm.movq_r_m(R.RAX, vsp_m2);
			asm.cqo();
			asm.idiv_r(I.r0);
			asm.movq_m_r(vsp_m2, R.RDX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.bind(done);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_REM_U); {
			asm.movq_r_m(I.r0, vsp_m1);
			saveReg(R.RAX);
			saveReg(R.RDX);
			asm.movq_r_m(R.RAX, vsp_m2);
			asm.movd_r_i(R.RDX, 0);
			asm.div_r(I.r0);
			asm.movq_m_r(vsp_m2, R.RDX);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I64_ADD, asm.q.add_m_r),
			(Opcode.I64_SUB, asm.q.sub_m_r),
			(Opcode.I64_AND, asm.q.and_m_r),
			(Opcode.I64_OR, asm.q.or_m_r),
			(Opcode.I64_XOR, asm.q.xor_m_r)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(I.r0, vsp_m1);
			t.1(vsp_m2, I.r0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I64_SHL, asm.q.shl_m_cl),
			(Opcode.I64_SHR_S, asm.q.sar_m_cl),
			(Opcode.I64_SHR_U, asm.q.shr_m_cl),
			(Opcode.I64_ROTL, asm.q.rol_m_cl),
			(Opcode.I64_ROTR, asm.q.ror_m_cl)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(I.r0, vsp_m1);
			t.1(vsp_m2);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genExtensions() {
		bindHandler(Opcode.I32_WRAP_I64); {
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I32_REINTERPRET_F32); {
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I64_REINTERPRET_F64); {
			genTagUpdate(BpTypeCode.I64.code);
			endHandler();
		}
		bindHandler(Opcode.F32_REINTERPRET_I32); {
			genTagUpdate(BpTypeCode.F32.code);
			endHandler();
		}
		bindHandler(Opcode.F64_REINTERPRET_I64); {
			genTagUpdate(BpTypeCode.F64.code);
			endHandler();
		}
		bindHandler(Opcode.I32_EXTEND8_S); {
			asm.d.movbsx_r_m(I.r0, vsp_m1);
			asm.movd_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I32_EXTEND16_S); {
			asm.d.movwsx_r_m(I.r0, vsp_m1);
			asm.movd_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND8_S); {
			asm.q.movbsx_r_m(I.r0, vsp_m1);
			asm.movq_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND16_S); {
			asm.q.movwsx_r_m(I.r0, vsp_m1);
			asm.movq_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND_I32_S);
		bindHandler(Opcode.I64_EXTEND32_S); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movd_r_m(I.r0, vsp_m1);
			asm.q.shl_r_i(I.r0, 32);
			asm.q.sar_r_i(I.r0, 32);
			asm.movq_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND_I32_U); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movd_m_i(IVar.VSP.gpr.plus(-4), 0); // zero upper portion
			endHandler();
		}
	}
	def genF32Arith() {
		bindHandler(Opcode.F32_ABS); {
			asm.d.and_m_i(vsp_m1, 0x7FFFFFFF); // explicit update of upper word
			endHandler();
		}
		bindHandler(Opcode.F32_NEG); {
			asm.d.xor_m_i(vsp_m1, 0x80000000); // explicit update of upper word
			endHandler();
		}
		bindHandler(Opcode.F32_ADD); {
			asm.movss_s_m(I.xmm0, vsp_m2);
			asm.addss_s_m(I.xmm0, vsp_m1);
			asm.movss_m_s(vsp_m2, I.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_SUB); {
			asm.movss_s_m(I.xmm0, vsp_m2);
			asm.subss_s_m(I.xmm0, vsp_m1);
			asm.movss_m_s(vsp_m2, I.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_MUL); {
			asm.movss_s_m(I.xmm0, vsp_m2);
			asm.mulss_s_m(I.xmm0, vsp_m1);
			asm.movss_m_s(vsp_m2, I.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_DIV); {
			asm.movss_s_m(I.xmm0, vsp_m2);
			asm.divss_s_m(I.xmm0, vsp_m1);
			asm.movss_m_s(vsp_m2, I.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_SQRT); {
			asm.sqrtss_s_m(I.xmm0, vsp_m1);
			asm.movss_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_COPYSIGN); {
			asm.movd_r_m(I.r0, vsp_m2); // XXX: tradeoff between memory operands and extra regs?
			asm.d.and_r_i(I.r0, 0x7FFFFFFF);
			asm.movd_r_m(I.r1, vsp_m1);
			asm.d.and_r_i(I.r1, 0x80000000);
			asm.d.or_r_r(I.r0, I.r1);
			asm.movd_m_r(vsp_m2, I.r0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.F32_CEIL, X86_64Rounding.TO_POS_INF),
			(Opcode.F32_FLOOR, X86_64Rounding.TO_NEG_INF),
			(Opcode.F32_TRUNC, X86_64Rounding.TO_ZERO),
			(Opcode.F32_NEAREST, X86_64Rounding.TO_NEAREST)
		]) {
			bindHandler(t.0);
			asm.movss_s_m(I.xmm0, vsp_m1);
			asm.roundss_s_s(I.xmm0, I.xmm0, t.1);
			asm.movss_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
	}
	def genF64Arith() {
		bindHandler(Opcode.F64_ABS); {
			asm.d.and_m_i(vsp_m1u, 0x7FFFFFFF);
			endHandler();
		}
		bindHandler(Opcode.F64_NEG); {
			asm.d.xor_m_i(vsp_m1u, 0x80000000);
			endHandler();
		}
		bindHandler(Opcode.F64_ADD); {
			asm.movsd_s_m(I.xmm0, vsp_m2);
			asm.addsd_s_m(I.xmm0, vsp_m1);
			asm.movsd_m_s(vsp_m2, I.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_SUB); {
			asm.movsd_s_m(I.xmm0, vsp_m2);
			asm.subsd_s_m(I.xmm0, vsp_m1);
			asm.movsd_m_s(vsp_m2, I.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_MUL); {
				asm.movsd_s_m(I.xmm0, vsp_m2);
				asm.mulsd_s_m(I.xmm0, vsp_m1);
				asm.movsd_m_s(vsp_m2, I.xmm0);
				asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_DIV); {
			asm.movsd_s_m(I.xmm0, vsp_m2);
			asm.divsd_s_m(I.xmm0, vsp_m1);
			asm.movsd_m_s(vsp_m2, I.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_SQRT); {
			asm.sqrtsd_s_m(I.xmm0, vsp_m1);
			asm.movsd_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_COPYSIGN); {
			asm.movd_r_m(I.r0, vsp_m2u); // XXX: tradeoff between memory operands and extra regs?
			asm.d.and_r_i(I.r0, 0x7FFFFFFF);
			asm.movd_r_m(I.r1, vsp_m1u);
			asm.d.and_r_i(I.r1, 0x80000000);
			asm.d.or_r_r(I.r0, I.r1);
			asm.movd_m_r(vsp_m2u, I.r0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.F64_CEIL, X86_64Rounding.TO_POS_INF),
			(Opcode.F64_FLOOR, X86_64Rounding.TO_NEG_INF),
			(Opcode.F64_TRUNC, X86_64Rounding.TO_ZERO),
			(Opcode.F64_NEAREST, X86_64Rounding.TO_NEAREST)
		]) {
			bindHandler(t.0);
			asm.movsd_s_m(I.xmm0, vsp_m1);
			asm.roundsd_s_s(I.xmm0, I.xmm0, t.1);
			asm.movsd_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
	}
	def genFloatCmps() {
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F32_EQ, C.NZ),
			(Opcode.F32_NE, C.Z),
			(Opcode.F32_LT, C.NC),
			(Opcode.F32_GT, C.NA),
			(Opcode.F32_LE, C.A),
			(Opcode.F32_GE, C.C)]) {
			bindHandler(t.0);
			asm.movss_s_m(I.xmm0, vsp_m2);
			asm.ucomiss_s_m(I.xmm0, vsp_m1);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F32_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsp_m1, 0);
		endHandler();

		asm.bind(ret_one);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsp_m1, 1);
		endHandler();

		// XXX: too far of a near jump to share these between f32 and f64
		ret_zero = X86_64Label.new();
		ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F64_EQ, C.NZ),
			(Opcode.F64_NE, C.Z),
			(Opcode.F64_LT, C.NC),
			(Opcode.F64_GT, C.NA),
			(Opcode.F64_LE, C.A),
			(Opcode.F64_GE, C.C)]) {
			bindHandler(t.0);
			asm.movsd_s_m(I.xmm0, vsp_m2);
			asm.ucomisd_s_m(I.xmm0, vsp_m1);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F64_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsp_m1, 0);
		genDispatchOrJumpToDispatch();

		asm.bind(ret_one);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsp_m1, 1);
		genDispatchOrJumpToDispatch();
	}
	def genMisc() {
		bindHandler(Opcode.MEMORY_SIZE); {
			genReadUleb32(I.r1);
			asm.movq_r_m(I.r0, IVar.INSTANCE.gpr.plus(offsets.Instance_memories));
			asm.movq_r_m(I.r0, I.r0.plusR(I.r1, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(I.r1, I.r0.plus(offsets.X86_64Memory_limit));
			asm.movq_r_m(I.r0, I.r0.plus(offsets.X86_64Memory_start));
			asm.q.sub_r_r(I.r1, I.r0);
			asm.q.shr_r_i(I.r1, 16);
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsp_p0, I.r1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.REF_NULL); {
			asm.movbzx_r_m(I.r0, I.ip_ptr);
			genSkipLeb(); // skip type
			genTagPushR(I.r0);
			asm.movq_m_i(vsp_p0, 0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.REF_IS_NULL); {
			asm.d.test_m_i(vsp_m1, -1);
			asm.set_r(C.Z, I.r0);
			asm.movbzx_r_r(I.r0, I.r0);
			if (valuerep.tagged) asm.movd_m_i(vsp_m1t, i7.view(BpTypeCode.I32.code));
			asm.movd_m_r(vsp_m1, I.r0);
			endHandler();
		}
		bindHandler(Opcode.REF_FUNC); {
			genReadUleb32(I.r1);
			asm.movq_r_m(I.r0, IVar.INSTANCE.gpr.plus(offsets.Instance_functions));
			asm.movq_r_m(I.r0, I.r0.plusR(I.r1, offsets.REF_SIZE, offsets.Array_contents));
			genTagPush(BpTypeCode.FUNCREF.code);
			asm.movq_m_r(vsp_p0, I.r0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.DATA_DROP); {
			genReadUleb32(I.r1);
			asm.movq_r_m(I.r0, IVar.INSTANCE.gpr.plus(offsets.Instance_dropped_data));
			asm.movb_m_i(I.r0.plusR(I.r1, 1, offsets.Array_contents), 1);
			endHandler();
		}
		bindHandler(Opcode.ELEM_DROP); {
			genReadUleb32(I.r1);
			asm.movq_r_m(I.r0, IVar.INSTANCE.gpr.plus(offsets.Instance_dropped_elems));
			asm.movb_m_i(I.r0.plusR(I.r1, 1, offsets.Array_contents), 1);
			endHandler();
		}
		bindHandler(Opcode.TABLE_SIZE); {
			genReadUleb32(I.r1);
			asm.movq_r_m(I.r0, IVar.INSTANCE.gpr.plus(offsets.Instance_tables));
			asm.movq_r_m(I.r0, I.r0.plusR(I.r1, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(I.r0, I.r0.plus(offsets.Table_elems));
			asm.movq_r_m(I.r0, I.r0.plus(offsets.Array_length));
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsp_p0, I.r0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		writeDispatchEntry(dispatchTables[0].1, InternalOpcode.PROBE.code, w.atEnd().pos); {
			saveCallerIVars();
			callRuntime(refRuntimeCall(this.i.runtime_PROBE_instr), [IVar.INSTANCE.gpr], true);
			restoreCallerIVars();
			// Compute a pointer to the original code at this pc offset
			var pc = I.r1; // = IP - CODE
			asm.movq_r_r(pc, IVar.IP.gpr);
			asm.sub_r_m(pc, R.RSP.plus(IVar.CODE.frameOffset));
			var origIp = I.r0; // FUNC_DECL.code.orig + pc - 1
			asm.movq_r_m(origIp, IVar.FUNC_DECL.gpr.plus(offsets.FuncDecl_code));
			asm.movq_r_m(origIp, origIp.plus(offsets.Code_orig));
			asm.add_r_r(origIp, pc);
			asm.sub_r_i(origIp, 1);
			genDispatch(origIp.indirect(), dispatchTables[0].1, false);
		}
		if (tuning.dispatchTableReg) {
			var offset = w.atEnd().pos;
			for (i < 256) {
				writeDispatchEntry(probedDispatchTableRef, i, offset);
			}
			saveCallerIVars();
			callRuntime(refRuntimeCall(this.i.runtime_PROBE_loop), [IVar.INSTANCE.gpr], true);
			restoreCallerIVars();
			// TODO: reload code from function, as local probes may have been inserted or removed
			asm.sub_r_i(IVar.IP.gpr, 1);
			genDispatch(IVar.IP.gpr.indirect(), dispatchTables[0].1, true);
		}
	}
	def genFloatMinAndMax() {
		var ret_b = X86_64Label.new(), ret_a = X86_64Label.new(), is_nan32 = X86_64Label.new(), is_nan64 = X86_64Label.new();
		bindHandler(Opcode.F32_MIN);
		asm.movss_s_m(I.xmm0, vsp_m2);
		asm.movss_s_m(I.xmm1, vsp_m1);
		asm.ucomiss_s_s(I.xmm0, I.xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(vsp_m1, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		bindHandler(Opcode.F32_MAX);
		asm.movss_s_m(I.xmm0, vsp_m2);
		asm.movss_s_m(I.xmm1, vsp_m1);
		asm.ucomiss_s_s(I.xmm0, I.xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(vsp_m1, 0);
		asm.jc_rel_near(C.NS, ret_b); // handle max(-0, 0) == 0
		asm.jmp_rel_near(ret_a);

		bindHandler(Opcode.F64_MIN);
		asm.movsd_s_m(I.xmm0, vsp_m2);
		asm.movsd_s_m(I.xmm1, vsp_m1);
		asm.ucomisd_s_s(I.xmm0, I.xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(vsp_m1u, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		// fall through to ret_a
		asm.bind(ret_a);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.F64_MAX);
		asm.movsd_s_m(I.xmm0, vsp_m2);
		asm.movsd_s_m(I.xmm1, vsp_m1);
		asm.ucomisd_s_s(I.xmm0, I.xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(vsp_m1u, 0);
		asm.jc_rel_near(C.S, ret_a); // handle max(-0, 0) == 0
		// fall through to ret_b
		asm.bind(ret_b);
		asm.movsd_m_s(vsp_m2, I.xmm1);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		asm.bind(is_nan32);
		asm.movd_m_i(vsp_m2, int.view(Floats.f_nan));
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan64);
		asm.movd_m_i(vsp_m2u, int.view(Floats.d_nan >> 32));
		asm.movd_m_i(vsp_m2, 0);
		asm.jmp_rel_near(ret_a);
	}
	def genFloatTruncs() {
		genFloatTrunc(Opcode.I32_TRUNC_F32_S, TRUNC_i32_f32_s, false);
		genFloatTrunc(Opcode.I32_TRUNC_F32_U, TRUNC_i32_f32_u, false);
		genFloatTrunc(Opcode.I32_TRUNC_F64_S, TRUNC_i32_f64_s, false);
		genFloatTrunc(Opcode.I32_TRUNC_F64_U, TRUNC_i32_f64_u, false);
		genFloatTrunc(Opcode.I64_TRUNC_F32_S, TRUNC_i64_f32_s, false);
		genFloatTrunc(Opcode.I64_TRUNC_F32_U, TRUNC_i64_f32_u, false);
		genFloatTrunc(Opcode.I64_TRUNC_F64_S, TRUNC_i64_f64_s, false);
		genFloatTrunc(Opcode.I64_TRUNC_F64_U, TRUNC_i64_f64_u, false);
		genFloatTrunc(Opcode.I32_TRUNC_SAT_F32_S, TRUNC_i32_f32_s, true);
		genFloatTrunc(Opcode.I32_TRUNC_SAT_F32_U, TRUNC_i32_f32_u, true);
		genFloatTrunc(Opcode.I32_TRUNC_SAT_F64_S, TRUNC_i32_f64_s, true);
		genFloatTrunc(Opcode.I32_TRUNC_SAT_F64_U, TRUNC_i32_f64_u, true);
		genFloatTrunc(Opcode.I64_TRUNC_SAT_F32_U, TRUNC_i64_f32_u, true);
		genFloatTrunc(Opcode.I64_TRUNC_SAT_F64_U, TRUNC_i64_f64_u, true);

		bindHandler(Opcode.I64_TRUNC_SAT_F32_S); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movss_s_m(I.xmm0, vsp_m1);
			asm.movd_r_i(I.r0, int.view(Floats.f_1p63));
			asm.movd_s_r(I.xmm1, I.r0);
			asm.ucomiss_s_s(I.xmm0, I.xmm1);
			var is_nan = X86_64Label.new(), ovf_pos = X86_64Label.new(), done = X86_64Label.new();
			asm.jc_rel_near(C.P, is_nan);
			asm.jc_rel_near(C.NC, ovf_pos);
			asm.roundss_s_s(I.xmm0, I.xmm0, X86_64Rounding.TO_ZERO);
			asm.q.cvtss2si_r_s(I.r0, I.xmm0);
			asm.bind(done);
			asm.movq_m_r(vsp_m1, I.r0);
			genDispatchOrJumpToDispatch();
			asm.bind(is_nan);
			asm.movd_r_i(I.r0, 0);
			asm.jmp_rel_near(done);
			asm.bind(ovf_pos);
			asm.movq_r_i(I.r0, 0xFFFFFFFE);  // TODO: tricky constant
			asm.q.ror_r_i(I.r0, 1); // result = 0x7FFFFFFF_FFFFFFFF
			asm.jmp_rel_near(done);
		}
		bindHandler(Opcode.I64_TRUNC_SAT_F64_S); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movsd_s_m(I.xmm0, vsp_m1);
			asm.movd_r_i(I.r0, int.view(Floats.d_1p63 >> 32));
			asm.q.shl_r_i(I.r0, 32);
			asm.movq_s_r(I.xmm1, I.r0);
			asm.ucomisd_s_s(I.xmm0, I.xmm1);
			var is_nan = X86_64Label.new(), ovf_pos = X86_64Label.new(), done = X86_64Label.new();
			asm.jc_rel_near(C.P, is_nan);
			asm.jc_rel_near(C.NC, ovf_pos);
			asm.roundsd_s_s(I.xmm0, I.xmm0, X86_64Rounding.TO_ZERO);
			asm.q.cvtsd2si_r_s(I.r0, I.xmm0);
			asm.bind(done);
			asm.movq_m_r(vsp_m1, I.r0);
			genDispatchOrJumpToDispatch();
			asm.bind(is_nan);
			asm.movd_r_i(I.r0, 0);
			asm.jmp_rel_near(done);
			asm.bind(ovf_pos);
			asm.movq_r_i(I.r0, 0xFFFFFFFE); // TODO: tricky constant
			asm.q.ror_r_i(I.r0, 1); // result = 0x7FFFFFFF_FFFFFFFF
			asm.jmp_rel_near(done);
			endHandler();
		}
	}
	def genFloatConversions() {
		bindHandler(Opcode.F32_CONVERT_I32_S); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movd_r_m(I.r0, vsp_m1);
			asm.q.shl_r_i(I.r0, 32);
			asm.q.sar_r_i(I.r0, 32); // sign-extend
			asm.cvtsi2ss_s_r(I.xmm0, I.r0);
			asm.movss_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I32_U); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movd_r_m(I.r0, vsp_m1);
			asm.cvtsi2ss_s_r(I.xmm0, I.r0);
			asm.movss_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I64_S); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movq_r_m(I.r0, vsp_m1);
			asm.cvtsi2ss_s_r(I.xmm0, I.r0);
			asm.movss_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I64_U); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movq_r_m(I.r0, vsp_m1);
			asm.q.cvtsi2ss_s_r(I.xmm0, I.r0);
			asm.q.cmp_r_i(I.r0, 0);
			var done = X86_64Label.new();
			asm.jc_rel_near(C.NS, done);
			// input < 0, compute 2.0d * cvt((x >> 1) | (x&1))
			asm.movq_r_r(I.r1, I.r0);
			asm.q.and_r_i(I.r1, 1);
			asm.q.shr_r_i(I.r0, 1);
			asm.q.or_r_r(I.r0, I.r1);
			asm.q.cvtsi2ss_s_r(I.xmm0, I.r0);
			asm.movd_r_i(I.r1, int.view(Floats.f_1p1));
			asm.movd_s_r(I.xmm1, I.r1);
			asm.mulss_s_s(I.xmm0, I.xmm1);
			// done
			asm.bind(done);
			asm.movss_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_DEMOTE_F64); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movsd_s_m(I.xmm0, vsp_m1);
			asm.cvtsd2ss_s_s(I.xmm0, I.xmm0);
			asm.movss_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I32_S); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movd_r_m(I.r0, vsp_m1);
			asm.q.shl_r_i(I.r0, 32);
			asm.q.sar_r_i(I.r0, 32); // sign-extend
			asm.cvtsi2sd_s_r(I.xmm0, I.r0);
			asm.movsd_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I32_U); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movd_r_m(I.r0, vsp_m1);
			asm.cvtsi2sd_s_r(I.xmm0, I.r0);
			asm.movsd_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I64_S); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movq_r_m(I.r0, vsp_m1);
			asm.cvtsi2sd_s_r(I.xmm0, I.r0);
			asm.movsd_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I64_U); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movq_r_m(I.r0, vsp_m1);
			asm.q.cvtsi2sd_s_r(I.xmm0, I.r0);
			asm.q.cmp_r_i(I.r0, 0);
			var done = X86_64Label.new();
			asm.jc_rel_near(C.NS, done);
			// input < 0, compute 2.0d * cvt((x >> 1) | (x&1))
			asm.movq_r_r(I.r1, I.r0);
			asm.q.and_r_i(I.r1, 1);
			asm.q.shr_r_i(I.r0, 1);
			asm.q.or_r_r(I.r0, I.r1);
			asm.q.cvtsi2sd_s_r(I.xmm0, I.r0);
			asm.movd_r_i(I.r1, int.view(Floats.d_1p1 >> 32));
			asm.q.shl_r_i(I.r1, 32);
			asm.movq_s_r(I.xmm1, I.r1);
			asm.mulsd_s_s(I.xmm0, I.xmm1);
			// done
			asm.bind(done);
			asm.movsd_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_PROMOTE_F32); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movss_s_m(I.xmm0, vsp_m1);
			asm.cvtss2sd_s_s(I.xmm0, I.xmm0);
			asm.movsd_m_s(vsp_m1, I.xmm0);
			endHandler();
		}
	}
	def genRuntimeCallOps() {
		// generate code for runtime calls with 1 LEB that cannot trap.
		var call_irt = asm.newLabel();
		for (t in [
			(Opcode.GLOBAL_GET, refRuntimeCall(i.runtime_GLOBAL_GET)),
			(Opcode.GLOBAL_SET, refRuntimeCall(i.runtime_GLOBAL_SET)),
			(Opcode.MEMORY_GROW, refRuntimeCall(i.runtime_MEMORY_GROW)),
			(Opcode.TABLE_GROW, refRuntimeCall(i.runtime_TABLE_GROW))
		]) {
			bindHandler(t.0);
			genReadUleb32(I.r0);
			saveCallerIVars();
			callRuntime(t.1, [IVar.INSTANCE.gpr, I.r0], false);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);

		// generate code for runtime calls with 1 LEB that can trap.
		call_irt = asm.newLabel();
		for (t in [
			(Opcode.TABLE_GET, refRuntimeCall(i.runtime_TABLE_GET)),
			(Opcode.TABLE_SET, refRuntimeCall(i.runtime_TABLE_SET)),
			(Opcode.MEMORY_FILL, refRuntimeCall(i.runtime_MEMORY_FILL)),
			(Opcode.TABLE_FILL, refRuntimeCall(i.runtime_TABLE_FILL))
		]) {
			bindHandler(t.0);
			genReadUleb32(I.r0);
			saveCallerIVars();
			callRuntime(t.1, [IVar.INSTANCE.gpr, I.r0], true);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);

		// generate code for runtime calls with 2 LEBS that can trap.
		call_irt = asm.newLabel();
		for (t in [
			(Opcode.TABLE_INIT, refRuntimeCall(i.runtime_TABLE_INIT)),
			(Opcode.MEMORY_INIT, refRuntimeCall(i.runtime_MEMORY_INIT)),
			(Opcode.MEMORY_COPY, refRuntimeCall(i.runtime_MEMORY_COPY)),
			(Opcode.TABLE_COPY, refRuntimeCall(i.runtime_TABLE_COPY))
		]) {
			bindHandler(t.0);
			genReadUleb32(I.r0);
			genReadUleb32(I.r1);
			saveCallerIVars();
			callRuntime(t.1, [IVar.INSTANCE.gpr, I.r0, I.r1], true);
			genExecStateCheck();
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);
	}
	def bindHandler(opcode: Opcode) {
		// TODO: if (tuning.handlerAlignment > 1) w.align(tuning.handlerAlignment);
		patchDispatchTable(opcode, w.atEnd().pos);
	}
	def genFloatTrunc(opcode: Opcode, config: FloatTrunc, saturate: bool) {
		bindHandler(opcode);
		config.mov_s_m(asm, I.xmm0, vsp_m1);
		config.mov_s_i(asm, I.xmm1, config.maxv);
		config.ucomi_s_s(asm, I.xmm0, I.xmm1);
		var above = X86_64Label.new(), is_nan = X86_64Label.new(), below = X86_64Label.new();
		var ret = X86_64Label.new();
		asm.jc_rel_near(C.P, is_nan);
		asm.jc_rel_near(C.NC, above);
		var not_big = X86_64Label.new();

		if (config.isI64 && !config.isSigned) {
			// handle u64 convert of 1p63 < v <= 1p64
			config.mov_s_i(asm, I.xmm1, if(config.isF64, Floats.d_1p63, Floats.f_1p63));
			config.ucomi_s_s(asm, I.xmm0, I.xmm1);
			asm.jc_rel_near(C.C, not_big);
			config.sub_s_s(asm, I.xmm0, I.xmm1);
			config.round_s_s(asm, I.xmm0, I.xmm0, X86_64Rounding.TO_ZERO);
			config.cvt2si_r_s(asm.q, I.r0, I.xmm0);
			asm.movd_r_i(I.r1, 1);
			asm.ror_r_i(I.r1, 1);
			asm.q.add_r_r(I.r0, I.r1);
			asm.jmp_rel_near(ret);
		}
		asm.bind(not_big);

		if (!saturate || config.isI64 || !config.isSigned) {
			config.mov_s_i(asm, I.xmm1, config.minv);
			config.ucomi_s_s(asm, I.xmm0, I.xmm1);
			asm.jc_rel_near(C.NA, below); // v <= min
		}

		config.round_s_s(asm, I.xmm0, I.xmm0, X86_64Rounding.TO_ZERO);
		if (!config.isI64 && config.isSigned) {
			config.cvt2si_r_s(asm.d, I.r0, I.xmm0);
		} else {
			config.cvt2si_r_s(asm.q, I.r0, I.xmm0);
		}
		asm.bind(ret);
		config.mov_m_r(asm, vsp_m1, I.r0);
		genTagUpdate(config.tag);
		genDispatchOrJumpToDispatch();
		if (saturate) {
			asm.bind(above);
			config.mov_r_i(asm, I.r0, config.ceilv);
			asm.jmp_rel_near(ret);
			asm.bind(is_nan);
			asm.bind(below);
			asm.movd_r_i(I.r0, 0);
			asm.jmp_rel_near(ret);
		} else {  // XXX: share trap code among all float truncs
			asm.bind(above);
			asm.bind(below);
			asm.bind(is_nan);
			genTrap(TrapReason.FLOAT_UNREPRESENTABLE);
		}
	}
	def genExecStateCheck() {
		asm.cmpb_r_i(Target.V3_RET_GPRS[0], ExecState.FINISHED.tag);
		asm.jc_rel_addr(C.NZ, abruptRetRef);
	}
	def saveCallerIVars() {
		saveIVar(IVar.IP);
		saveIVar(IVar.STP);
	}
	def restoreCallerIVars() {
		restoreIVar(IVar.IP);
		restoreIVar(IVar.STP);
		restoreIVar(IVar.EIP);
		restoreIVar(IVar.INSTANCE);
		restoreIVar(IVar.FUNC_DECL);
		restoreIVar(IVar.MEM0_BASE);
		restoreIVar(IVar.VFP);
	}
	def callRuntime(abs: Pointer, regs: Array<X86_64Gpr>, canTrap: bool) {
		saveIVar(IVar.VSP);
		// Generate parallel moves from args into param gprs; assume each src register used only once
		var dst = Array<X86_64Gpr>.new(G.length);
		for (i < regs.length) {
			var sreg = regs[i];
			var dreg = Target.V3_PARAM_GPRS[i + 1];
			if (sreg != dreg) dst[sreg.regnum] = dreg;
		}
		var stk = Array<i8>.new(G.length);
		for (i < dst.length) orderMoves(dst, stk, i);
		// save a copy of VSP into valueStack.sp
		asm.movq_r_m(I.scratch, absPointer(offsets.Interpreter_valueStack));
		asm.movq_r_m(R.R14, R.RSP.plus(IVar.VSP.frameOffset)); // TODO: use other register than R14
		asm.movq_m_r(I.scratch.plus(offsets.ValueStack_sp), R.R14);
		// emit actual call
		asm.callr(int.view(u32.!(abs - (ic.start + w.pos + 5))));
		// check for trap
		if (canTrap) genExecStateCheck();
		if (tuning.dispatchTableReg) {
			// restore dispatch table from interpreter.dispatchTable
			asm.movq_r_m(IVar.DISPATCH_TABLE.gpr, absPointer(offsets.Interpreter_dispatchTable));
		}
		// restore VSP from valueStack.sp
		asm.movq_r_m(IVar.VSP.gpr, absPointer(offsets.Interpreter_valueStack));
		asm.movq_r_m(IVar.VSP.gpr, IVar.VSP.gpr.plus(offsets.ValueStack_sp));
	}
	def absPointer(ptr: Pointer) -> X86_64Addr {
		return X86_64Addr.new(null, null, 1, int.!(ptr - Pointer.NULL));
	}
	def orderMoves(dst: Array<X86_64Gpr>, stk: Array<i8>, i: int) {
		var dreg = dst[i];
		if (dreg == null) return;		// no moves here
		if (stk[i] > 0) return;			// this node already done
		stk[i] = -1;				// mark as on stack
		if (stk[dreg.regnum] < 0) {		// destination on stack => cycle
			asm.movq_r_r(I.scratch, dreg);	// save destination first
			stk[dreg.regnum] = -2;		// mark as cycle
		} else {
			orderMoves(dst, stk, dreg.regnum);	// recurse on destination
		}
		asm.movq_r_r(dreg, if(stk[i] == -2, I.scratch, G[i]));	// emit post-order move
		stk[i] = 1;				// mark as done
	}
	def genTagUpdate(tag: byte) {
		if (valuerep.tagged) asm.movq_m_i(vsp_m1t, tag);
	}
	def genTagPush(tag: byte) {
		if (valuerep.tagged) asm.movq_m_i(vsp_p0t, i7.view(tag));
	}
	def genTagPushR(r: X86_64Gpr) {
		if (valuerep.tagged) asm.movq_m_r(vsp_p0t, r);
	}
	def genCopySlot(dst: X86_64Addr, src: X86_64Addr) {
		if (valuerep.slot_size == 16) {
			asm.movdqu_s_m(I.xmm0, src);
			asm.movdqu_m_s(dst, I.xmm0);
		} else {
			asm.movq_r_m(I.scratch, src);
			asm.movq_m_r(dst, I.scratch);
		}
	}
	def saveIVar(iv: IVar) {
		asm.movq_m_r(R.RSP.plus(iv.frameOffset), iv.gpr);
	}
	def restoreIVar(iv: IVar) {
		asm.movq_r_m(iv.gpr, R.RSP.plus(iv.frameOffset));
	}
	def saveReg(r: X86_64Gpr) {
		for (iv in IVar) {
			if (iv.gpr == r) {
				if (iv.mutable) asm.movq_m_r(R.RSP.plus(iv.frameOffset), r);
				break;
			}
		}
	}
	def restoreReg(r: X86_64Gpr) {
		for (iv in IVar) {
			if (iv.gpr == r) {
				asm.movq_r_m(r, R.RSP.plus(iv.frameOffset));
				break;
			}
		}
	}
	def genLoad(opcode: Opcode, tag: byte, gen: (X86_64Gpr, X86_64Addr) -> X86_64Assembler) {
		bindHandler(opcode);
		asm.q.inc_r(I.ip);			// skip flags byte
		genReadUleb32(I.r0);			// decode offset
		asm.movd_r_m(I.r1, vsp_m1);		// read index
		asm.q.add_r_r(I.r0, I.r1);		// add index + offset
		gen(I.r1, IVar.MEM0_BASE.gpr.plusR(I.r0, 1, 0));
		if (valuerep.tagged && tag != BpTypeCode.I32.code) genTagUpdate(tag); // update tag if necessary
		asm.movq_m_r(vsp_m1, I.r1);
		endHandler();
	}
	def genStore(gen: (X86_64Addr, X86_64Gpr) -> X86_64Assembler) {
		asm.q.inc_r(I.ip);			// skip flags byte
		genReadUleb32(I.r0);			// decode offset
		asm.movd_r_m(I.r1, vsp_m2);		// read index
		asm.q.add_r_r(I.r0, I.r1);		// add index + offset
		asm.movq_r_m(I.r1, vsp_m1);		// read value
		gen(IVar.MEM0_BASE.gpr.plusR(I.r0, 1, 0), I.r1);
		asm.q.sub_r_i(vsp, 2 * valuerep.slot_size);
		endHandler();
	}
	def genTrap(reason: TrapReason) {
		asm.movd_r_i(Target.V3_RET_GPRS[0], ExecState.TRAPPED.tag);
		asm.movd_r_i(Target.V3_RET_GPRS[1], reason.tag);
		abruptRetRef = IcCodeRef.new(w.pos);
		asm.q.add_r_i(R.RSP, ic.frameSize);
		asm.ret();
	}

	// Generate a read of a 32-bit unsigned LEB.
	def genReadUleb32(dest: X86_64Gpr) {
		var ool_leb: OutOfLineLEB;
		if (!tuning.inlineAllLEBs) {
			ool_leb = OutOfLineLEB.new(dest);
			oolULeb32Sites.put(ool_leb);
		}
		var asm = this.asm.d;
		asm.movbzx_r_m(dest, I.ip_ptr);		// load first byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.test_r_i(dest, 0x80);		// test most-significant bit
		if (tuning.inlineAllLEBs) {
			var leb_done = X86_64Label.new();
			asm.jc_rel_near(C.Z, leb_done);
			genReadLEBext(dest);
			asm.bind(leb_done);
		} else {
			asm.jc_rel_addr(C.NZ, ool_leb);
			ool_leb.retOffset = asm.pos();
		}
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb32_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(I.scratch, I.ip_ptr);	// load byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.d.test_r_i(I.scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(I.scratch, 0x7F);		// mask off upper bit
		asm.d.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.d.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.sub_r_i(R.RCX, 25);		// compute 25 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 25, done
		asm.d.shl_r_cl(dest);			// sign extension
		asm.d.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb64_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(I.scratch, I.ip_ptr);	// load byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.d.test_r_i(I.scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(I.scratch, 0x7F);		// mask off upper bit
		asm.q.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.q.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.sub_r_i(R.RCX, 57);		// compute 57 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 57, done
		asm.q.shl_r_cl(dest);			// sign extension
		asm.q.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate code which skips over an LEB.
	def genSkipLeb() {
		var more = X86_64Label.new();
		asm.bind(more);
		asm.movbzx_r_m(I.scratch, I.ip_ptr);	// load first byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.test_r_i(I.scratch, 0x80);		// test most-significant bit
		asm.jc_rel_near(C.NZ, more);
	}
	// End the handler for the current bytecode
	def endHandler() {
		genDispatchOrJumpToDispatch();
	}
	// Generate an inline dispatch or a jump to the dispatch loop, depending on config.
	def genDispatchOrJumpToDispatch() {
		if (tuning.threadedDispatch) genDispatch(I.ip_ptr, if (!tuning.dispatchTableReg, dispatchTables[0].1), true);
		else asm.jmp_rel(firstDispatchOffset - w.atEnd().pos);
	}
	// Generate a load of the next bytecode and a dispatch through the dispatch table.
	def genDispatch(ptr: X86_64Addr, table: IcCodeRef, increment: bool) {
		var opcode = I.r0;
		var base = I.r1;
		if (ptr != null) asm.movbzx_r_m(opcode, ptr);
		if (increment) asm.inc_r(I.ip);
		match (tuning.dispatchEntrySize) {
			2 => {
				if (table == null) asm.movq_r_r(base, IVar.DISPATCH_TABLE.gpr);
				else asm.lea(base, table); // RIP-relative LEA
				asm.movwsx_r_m(opcode, base.plusR(opcode, 2, 0)); // load 16-bit offset
				asm.add_r_r(base, opcode);
				if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
				asm.ijmp_r(base);
			}
			4 => {
				if (table == null) {
					asm.movd_r_m(base, IVar.DISPATCH_TABLE.gpr.plusR(opcode, 4, 0));
				} else {
					var addr = ic.start + table.offset;
					asm.movd_r_m(base, X86_64Addr.new(null, opcode, 4, int.!(addr - Pointer.NULL)));
				}
				if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
				asm.ijmp_r(base);
			}
			8 => {
				if (table == null) {
					if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
					asm.ijmp_m(IVar.DISPATCH_TABLE.gpr.plusR(opcode, 8, 0));
				} else {
					var addr = ic.start + table.offset;
					if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
					asm.ijmp_m(X86_64Addr.new(null, opcode, 8, int.!(addr - Pointer.NULL)));
				}
			}
		}
	}
	// Patch the dispatch table for the given opcode to go to the given position.
	def patchDispatchTable(opcode: Opcode, pos: int) {
		for (t in dispatchTables) {
			if (t.0 != opcode.prefix) continue;
			var ref1 = t.1;
			if (opcode.prefix == 0 || opcode.code < 128) writeDispatchEntry(ref1, opcode.code, pos);
			var ref2 = t.2;
			if (ref2 != null) writeDispatchEntry(ref2, opcode.code, pos);
			w.atEnd();
			return;
		}
		fatal("no dispatch table found for prefix");
	}
	// Generate the out-of-line LEB decoding code.
	def genOutOfLineLEBs() { // XXX: use a separate out-of-line assembler on the end of the buffer
		for (i < oolULeb32Sites.length) {
			var o = oolULeb32Sites[i];
			var pos = w.atEnd().pos;
			w.at(o.pos).put_b32(pos - (o.pos + o.delta));
			w.atEnd();
			// XXX: share code between out-of-line LEB cases
			genReadLEBext(o.dest);
			asm.jmp_rel(o.retOffset - w.atEnd().pos);
		}
		oolULeb32Sites = null;
	}
	// Generate code for > 1 byte LEB cases
	def genReadLEBext(dest: X86_64Gpr) {
		var destRcx = dest == R.RCX;
		asm.d.and_r_i(dest, 0x7F);		// mask off upper bit of first byte
		if (destRcx) {
			asm.movd_r_r(I.r3, dest);
			dest = I.r3;
		} else {
			asm.movd_r_r(I.r4, R.RCX);	// save RCX
		}
		asm.movd_r_i(R.RCX, 7);
		var loop = X86_64Label.new(), nomore = X86_64Label.new();
		asm.bind(loop);
		asm.movbzx_r_m(I.scratch, I.ip_ptr);	// load byte
		asm.q.inc_r(I.ip);			// increment pointer
		asm.d.test_r_i(I.scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.Z, nomore);		// break if not set
		asm.d.and_r_i(I.scratch, 0x7F);		// mask off upper bit
		asm.d.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, I.scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(nomore);
		asm.d.shl_r_cl(I.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, I.scratch);		// merge byte into val
		if (destRcx) asm.movd_r_r(R.RCX, dest);
		else asm.movd_r_r(R.RCX, I.r4);		// restore RCX
	}
	// Generate trap handler code.
	def genTrapHandlers() {
		w.atEnd();
		ic.oobMemoryHandlerOffset = w.pos;
		genTrap(TrapReason.MEM_OUT_OF_BOUNDS);

		ic.divZeroHandlerOffset = w.pos;
		genTrap(TrapReason.DIV_BY_ZERO);

		ic.stackOverflowHandlerOffset = w.pos;
		genTrap(TrapReason.STACK_OVERFLOW);
	}
	def refRuntimeCall<P, R>(f: P -> R) -> Pointer {
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(f).0;
		var abs = ptr - Pointer.NULL;
		if (abs > u32.max) fatal("runtime call address not in 4GB");
		return ptr;
	}
	def reportOom(w: DataWriter, nlength: int) -> DataWriter {
		fatal("ran out of buffer space");
		return w;
	}
}
type HandlerGenResult {
	case UNHANDLED;
	case HANDLED;
	case Offset(off: int);
	case END;
}

// Assembler patching support for out-of-line LEBs and other code refs.
def ABS_MARKER = 0x55443322;
def REL_MARKER = 0x44332211;
class OutOfLineLEB(dest: X86_64Gpr) extends X86_64Addr {
	var retOffset: int; // where OOB code should "return"
	var pos: int = -1;
	var delta: int;

	new() super(null, null, 1, REL_MARKER) { }
}
class IcCodeRef(var offset: int) extends X86_64Addr {
	new() super(null, null, 1, REL_MARKER) { }
}
class Patcher(w: DataWriter) extends X86_64AddrPatcher {
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		match (addr) {
			x: OutOfLineLEB => {
				x.pos = pos;
				x.delta = delta;
			}
			x: IcCodeRef => {
				if (x.offset < 0) System.error("InterpreterGen", "unbound forward code ref");
				w.at(pos).put_b32(x.offset - (pos + delta));
				w.atEnd();
			}
		}
	}
}
// A utility that generates constants and picks appropriate instructions for rounding, data movement,
// and conversion in dealing with floating point truncations.
class FloatTrunc(isI64: bool, isF64: bool, isSigned: bool) {
	def round_s_s = if(isF64, X86_64Assembler.roundsd_s_s, X86_64Assembler.roundss_s_s);
	def sub_s_s = if(isF64, X86_64Assembler.subsd_s_s, X86_64Assembler.subss_s_s);
	def ucomi_s_s = if(isF64, X86_64Assembler.ucomisd_s_s, X86_64Assembler.ucomiss_s_s);
	def mov_s_r = if(isF64, X86_64Assembler.movq_s_r, X86_64Assembler.movd_s_r);
	def mov_s_m = if(isF64, X86_64Assembler.movsd_s_m, X86_64Assembler.movss_s_m);
	def mov_m_s = if(isF64, X86_64Assembler.movsd_m_s, X86_64Assembler.movss_m_s);
	def mov_m_r = if(isI64, X86_64Assembler.movq_m_r, X86_64Assembler.movd_m_r);
	def maxv: u64 = if(isI64,
				if(isSigned,
					if(isF64, Floats.d_1p63, Floats.f_1p63),
					if(isF64, Floats.d_1p64, Floats.f_1p64)),
				if(isSigned,
					if(isF64, Floats.d_1p31, Floats.f_1p31),
					if(isF64, Floats.d_1p32, Floats.f_1p32)));
	def minv: u64 = if(isI64, // XXX: share these constants with V3 interpreter
				if(isSigned,
					if(isF64, u64.view(-9.223372036854778E18d), u32.view(-9.223373e18f)),
					if(isF64, u64.view(-1d), u32.view(-1f))),
				if(isSigned,
					if(isF64, u64.view(-2147483649d), u32.view(-2.1474839E9f)),
					if(isF64, u64.view(-1d), u32.view(-1f))));

	def minus1: u64 = if(isF64, Floats.d_minus1, Floats.f_minus1);

	def ceilv: u64 = if(isI64,
				if(isSigned, u63.max, u64.max),
				if(isSigned, u31.max, u32.max));
	def floorv: u64 = if(isSigned,
				if(isI64, u64.view(i63.min), u64.view(i31.min)));

	def tag = if(isI64, BpTypeCode.I64, BpTypeCode.I32).code;

	def mov_s_i(asm: X86_64Assembler, s: X86_64Xmmr, v: u64) {
		if (isF64) {
			if ((v & u32.max) == 0) {
				asm.movd_r_i(I.scratch, int.view(v >> 32));
				asm.q.shl_r_i(I.scratch, 32);
				asm.movq_s_r(s, I.scratch);
			} else if (int.view(v) > 0) { // no sign extension
				// XXX: load float constants from memory
				asm.movd_r_i(I.scratch, int.view(v >> 32));
				asm.q.shl_r_i(I.scratch, 32);
				asm.q.or_r_i(I.scratch, int.view(v));
				asm.movq_s_r(s, I.scratch);
			} else {
				System.error("FloatTrunc", "tricky 64-bit constant unimplemented");
			}
		} else {
			asm.movd_r_i(I.scratch, int.view(v));
			asm.movd_s_r(s, I.scratch);
		}
	}
	def mov_r_i(asm: X86_64Assembler, r: X86_64Gpr, v: u64) {
		if (isI64) {
			if (i32.view(v) == i64.view(v)) {
				asm.movq_r_i(r, int.view(v));
			} else if (u32.view(v) == u64.view(v)) {
				asm.movd_r_i(r, int.view(v));
			} else {
				System.error("FloatTrunc", "tricky 64-bit constant unimplemented");
			}
		} else {
			asm.movd_r_i(r, int.view(v));
		}
	}
	def cvt2si_r_s = if(isF64, X86_64Assembler.cvtsd2si_r_s, X86_64Assembler.cvtss2si_r_s);
}
def TRUNC_i32_f32_s = FloatTrunc.new(false, false, true);
def TRUNC_i32_f32_u = FloatTrunc.new(false, false, false);
def TRUNC_i32_f64_s = FloatTrunc.new(false, true, true);
def TRUNC_i32_f64_u = FloatTrunc.new(false, true, false);
def TRUNC_i64_f32_s = FloatTrunc.new(true, false, true);
def TRUNC_i64_f32_u = FloatTrunc.new(true, false, false);
def TRUNC_i64_f64_s = FloatTrunc.new(true, true, true);
def TRUNC_i64_f64_u = FloatTrunc.new(true, true, false);
